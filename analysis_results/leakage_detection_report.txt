Data Leakage Detection Report
============================

Analysis Timestamp: 2025-10-31 00:04:05.134519
Overall Leakage Score: 0.2337 (0=no leakage, 1=maximum leakage)

Records Analyzed:
  train: 10,000 records
  validation: 10,000 records
  test: 10,000 records

Duplicate Content Detection:
  Found 145 exact duplicates across splits
    Content hash 2d60aa3b... appears in: train, test, validation
    Content hash ffdec160... appears in: train, validation
    Content hash 424b5765... appears in: train, test, validation
    Content hash efb1ed50... appears in: train, test
    Content hash 9b23db7f... appears in: train, test, validation
    Content hash 41236b72... appears in: train, validation
    Content hash eee3be2a... appears in: train, test, validation
    Content hash 8cbac9e6... appears in: train, validation
    Content hash 8ae00dbc... appears in: train, validation
    Content hash 27baef56... appears in: train, test
    ... and 135 more duplicates

Temporal Consistency Analysis:
  Found 2 temporal issues
    future_information_in_training: ['train', 'validation'] - Severity: high
      Temporal overlap: 3232 days
    future_information_in_training: ['train', 'test'] - Severity: high
      Temporal overlap: 3791 days

Metadata Leakage Detection:
  Found potential metadata leakage in 3 splits
    train:
      - Column 'score' contains unrealistic values suggesting post-hoc data
      - Column 'num_comments' contains unrealistic values suggesting post-hoc data
    validation:
      - Column 'score' contains unrealistic values suggesting post-hoc data
    test:
      - Column 'score' contains unrealistic values suggesting post-hoc data

Near-Duplicate Content Detection:
  Found 845 near-duplicate pairs
    train vs validation: 1.000 similarity
      Preview 1: cutouts cutouts...
      Preview 2: cutouts cutouts...
    train vs validation: 1.000 similarity
      Preview 1: cutouts cutouts...
      Preview 2: cutouts cutouts...
    train vs validation: 1.000 similarity
      Preview 1: cutouts cutouts...
      Preview 2: cutouts cutouts...
    train vs validation: 1.000 similarity
      Preview 1: cutouts cutouts...
      Preview 2: cutouts cutouts...
    train vs validation: 1.000 similarity
      Preview 1: cutouts cutouts...
      Preview 2: cutouts cutouts...
    ... and 840 more pairs

Recommended Mitigation Strategies:
  1. Remove 145 exact duplicate content items across splits
  2. Implement content deduplication pipeline before train/validation/test splitting
  3. Implement strict temporal splitting to ensure training data predates validation/test data
  4. Add temporal buffer period between training and evaluation data
  5. Remove or mask metadata columns that contain post-publication information
  6. Validate that all features would be available at prediction time
  7. Consider removing or capping engagement metrics (score, comments)
  8. Review and potentially remove 845 near-duplicate pairs
  9. Implement fuzzy deduplication with similarity threshold < 0.85
  10. Implement automated leakage detection in data preprocessing pipeline
  11. Document data collection and splitting methodology
