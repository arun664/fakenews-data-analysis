""" Data Leakage Detection and Validation Module This module implements comprehensive data leakage detection to identify potential information leakage between train/validation/test splits, ensuring data integrity for pattern analysis. Requirements addressed: 1.4, 1.5 """ import pandas as pd import numpy as np import logging from typing import Dict, List, Optional, Tuple, Set, Any from dataclasses import dataclass from datetime import datetime, timedelta from pathlib import Path import hashlib from collections import defaultdict import re logger = logging.getLogger(__name__) @dataclass class LeakageReport: """Data leakage detection report""" total_records_checked: Dict[str, int] duplicate_content: Dict[str, List[str]] temporal_inconsistencies: List[Dict[str, Any]] metadata_leakage: Dict[str, List[str]] near_duplicate_pairs: List[Dict[str, Any]] leakage_score: float mitigation_strategies: List[str] timestamp: datetime class DataLeakageDetector: """ Detects and validates potential data leakage between dataset splits. This class implements comprehensive leakage detection including: - Duplicate content detection across splits - Temporal consistency validation - Metadata leakage identification - Near-duplicate content detection """ def __init__(self, similarity_threshold: float = 0.85, temporal_buffer_days: int = 1): """ Initialize data leakage detector. Args: similarity_threshold: Threshold for near-duplicate detection (0-1) temporal_buffer_days: Buffer days for temporal consistency checks """ self.similarity_threshold = similarity_threshold self.temporal_buffer_days = temporal_buffer_days self.leakage_thresholds = { 'duplicate_threshold': 0.01, # 1% duplicates threshold 'temporal_threshold': 0.005, # 0.5% temporal inconsistencies 'metadata_threshold': 0.02 # 2% metadata leakage threshold } def detect_data_leakage(self, train_df: pd.DataFrame, validation_df: pd.DataFrame, test_df: pd.DataFrame) -> LeakageReport: """ Perform comprehensive data leakage detection across all splits. Args: train_df: Training dataset validation_df: Validation dataset test_df: Test dataset Returns: LeakageReport with detection results Requirements: 1.4, 1.5 """ logger.info("Starting comprehensive data leakage detection") splits = { 'train': train_df, 'validation': validation_df, 'test': test_df } # 1. Detect duplicate content across splits duplicate_content = self._detect_duplicate_content(splits) # 2. Analyze temporal consistency temporal_inconsistencies = self._analyze_temporal_consistency(splits) # 3. Check for metadata leakage metadata_leakage = self._detect_metadata_leakage(splits) # 4. Find near-duplicate content near_duplicate_pairs = self._find_near_duplicates(splits) # 5. Calculate overall leakage score leakage_score = self._calculate_leakage_score( duplicate_content, temporal_inconsistencies, metadata_leakage, near_duplicate_pairs, splits ) # 6. Generate mitigation strategies mitigation_strategies = self._generate_mitigation_strategies( duplicate_content, temporal_inconsistencies, metadata_leakage, near_duplicate_pairs ) report = LeakageReport( total_records_checked={split: len(df) for split, df in splits.items()}, duplicate_content=duplicate_content, temporal_inconsistencies=temporal_inconsistencies, metadata_leakage=metadata_leakage, near_duplicate_pairs=near_duplicate_pairs, leakage_score=leakage_score, mitigation_strategies=mitigation_strategies, timestamp=datetime.now() ) logger.info(f"Data leakage detection completed. Leakage score: {leakage_score:.3f}") return report def _detect_duplicate_content(self, splits: Dict[str, pd.DataFrame]) -> Dict[str, List[str]]: """ Detect exact duplicate content across different splits. Args: splits: Dictionary of split name to DataFrame Returns: Dictionary mapping content hashes to split names where they appear """ logger.info("Detecting duplicate content across splits") content_hash_map = defaultdict(list) duplicate_content = {} # Create content hashes for each split for split_name, df in splits.items(): logger.info(f"Processing {split_name} split for duplicate detection") # Use multiple columns for content identification content_columns = ['clean_title', 'title', 'selftext'] available_columns = [col for col in content_columns if col in df.columns] if not available_columns: logger.warning(f"No suitable content columns found in {split_name} split") continue # Create combined content hash for idx, row in df.iterrows(): combined_content = "" for col in available_columns: if pd.notna(row.get(col)): combined_content += str(row[col]).strip().lower() if combined_content: content_hash = hashlib.md5(combined_content.encode()).hexdigest() content_hash_map[content_hash].append(split_name) # Identify hashes that appear in multiple splits for content_hash, split_list in content_hash_map.items(): unique_splits = list(set(split_list)) if len(unique_splits) > 1: duplicate_content[content_hash] = unique_splits logger.info(f"Found {len(duplicate_content)} duplicate content items across splits") return duplicate_content def _analyze_temporal_consistency(self, splits: Dict[str, pd.DataFrame]) -> List[Dict[str, Any]]: """ Analyze temporal consistency to ensure no future information in training data. Args: splits: Dictionary of split name to DataFrame Returns: List of temporal inconsistency issues """ logger.info("Analyzing temporal consistency across splits") temporal_issues = [] # Check if temporal columns exist temporal_columns = ['created_utc', 'timestamp'] temporal_stats = {} for split_name, df in splits.items(): available_temporal_cols = [col for col in temporal_columns if col in df.columns] if not available_temporal_cols: logger.warning(f"No temporal columns found in {split_name} split") continue temporal_col = available_temporal_cols[0] # Convert to datetime if needed df_copy = df.copy() if not pd.api.types.is_datetime64_any_dtype(df_copy[temporal_col]): try: if df_copy[temporal_col].dtype == 'int64' or df_copy[temporal_col].dtype == 'float64': # Assume Unix timestamp df_copy[temporal_col] = pd.to_datetime(df_copy[temporal_col], unit='s', errors='coerce') else: df_copy[temporal_col] = pd.to_datetime(df_copy[temporal_col], errors='coerce') except Exception as e: logger.warning(f"Could not convert temporal column in {split_name}: {e}") continue # Store temporal statistics for cross-split comparison valid_timestamps = df_copy[temporal_col].dropna() if len(valid_timestamps) > 0: # Ensure we have proper datetime objects min_date = pd.to_datetime(valid_timestamps.min()) max_date = pd.to_datetime(valid_timestamps.max()) median_date = pd.to_datetime(valid_timestamps.median()) temporal_stats[split_name] = { 'min_date': min_date, 'max_date': max_date, 'median_date': median_date } # Compare temporal ranges across splits split_names = ['train', 'validation', 'test'] for i, split1 in enumerate(split_names): for split2 in split_names[i+1:]: if split1 in temporal_stats and split2 in temporal_stats: stats1 = temporal_stats[split1] stats2 = temporal_stats[split2] # Check for temporal overlap that might indicate leakage if split1 == 'train' and split2 in ['validation', 'test']: # Training data should not contain future information if stats1['max_date'] > stats2['min_date']: time_diff = stats1['max_date'] - stats2['min_date'] overlap_days = time_diff.days if overlap_days > self.temporal_buffer_days: temporal_issues.append({ 'issue_type': 'future_information_in_training', 'splits': [split1, split2], 'train_max_date': stats1['max_date'], 'target_min_date': stats2['min_date'], 'overlap_days': overlap_days, 'severity': 'high' if overlap_days > 30 else 'medium' }) # Check for unrealistic temporal gaps time_diff = abs(stats1['median_date'] - stats2['median_date']) gap_days = time_diff.days if gap_days > 365: # More than 1 year gap temporal_issues.append({ 'issue_type': 'large_temporal_gap', 'splits': [split1, split2], 'gap_days': gap_days, 'severity': 'low' }) logger.info(f"Found {len(temporal_issues)} temporal consistency issues") return temporal_issues def _detect_metadata_leakage(self, splits: Dict[str, pd.DataFrame]) -> Dict[str, List[str]]: """ Detect metadata that might contain target information not available in real scenarios. Args: splits: Dictionary of split name to DataFrame Returns: Dictionary of potential metadata leakage issues """ logger.info("Detecting potential metadata leakage") metadata_leakage = {} # Define suspicious metadata columns that might leak target information suspicious_columns = [ 'score', # Post score might correlate with authenticity 'num_comments', # Comment count might indicate engagement patterns 'upvote_ratio', # Voting patterns might reveal authenticity 'gilded', # Awards might correlate with content quality 'stickied', # Moderation actions might indicate content issues 'locked', # Locking might indicate problematic content 'removed', # Removal status directly indicates content issues 'deleted', # Deletion status might indicate content issues ] for split_name, df in splits.items(): split_issues = [] for col in suspicious_columns: if col in df.columns: # Check if this metadata shows strong correlation with labels if self._check_metadata_target_correlation(df, col): split_issues.append(f"Column '{col}' shows high correlation with target labels") # Check for unrealistic values that might indicate post-hoc information if col in ['score', 'num_comments']: if self._check_unrealistic_engagement_values(df, col): split_issues.append(f"Column '{col}' contains unrealistic values suggesting post-hoc data") # Check for columns that shouldn't be available at prediction time prediction_time_issues = self._check_prediction_time_availability(df) split_issues.extend(prediction_time_issues) if split_issues: metadata_leakage[split_name] = split_issues logger.info(f"Found metadata leakage issues in {len(metadata_leakage)} splits") return metadata_leakage def _check_metadata_target_correlation(self, df: pd.DataFrame, column: str) -> bool: """Check if metadata column shows suspicious correlation with target labels""" # Look for target label columns target_columns = ['2_way_label', '6_way_label', 'authenticity_label', 'category_label'] available_targets = [col for col in target_columns if col in df.columns] if not available_targets or column not in df.columns: return False target_col = available_targets[0] try: # Calculate correlation for numeric columns if pd.api.types.is_numeric_dtype(df[column]) and pd.api.types.is_numeric_dtype(df[target_col]): correlation = abs(df[column].corr(df[target_col])) return correlation > 0.3 # Threshold for suspicious correlation # For categorical targets, check if metadata values cluster by category if not pd.api.types.is_numeric_dtype(df[target_col]): grouped_stats = df.groupby(target_col)[column].agg(['mean', 'std']).reset_index() if len(grouped_stats) > 1: # Check if means are significantly different across categories mean_range = grouped_stats['mean'].max() - grouped_stats['mean'].min() overall_std = df[column].std() return mean_range > 2 * overall_std # Significant difference threshold except Exception as e: logger.debug(f"Could not check correlation for {column}: {e}") return False def _check_unrealistic_engagement_values(self, df: pd.DataFrame, column: str) -> bool: """Check for unrealistic engagement values that suggest post-hoc data collection""" if column not in df.columns: return False try: values = df[column].dropna() if len(values) == 0: return False # Check for extreme outliers that might indicate post-hoc collection q99 = values.quantile(0.99) q95 = values.quantile(0.95) # If 99th percentile is much higher than 95th, might indicate post-hoc data if q99 > 10 * q95 and q95 > 0: return True # Check for negative values in engagement metrics if (values < 0).any(): return True except Exception as e: logger.debug(f"Could not check unrealistic values for {column}: {e}") return False def _check_prediction_time_availability(self, df: pd.DataFrame) -> List[str]: """Check for columns that wouldn't be available at prediction time""" issues = [] # Columns that typically aren't available when making predictions unavailable_columns = [ 'removed', # Removal happens after posting 'deleted', # Deletion happens after posting 'locked', # Locking happens after posting 'archived', # Archiving happens much later 'gilded', # Awards are given after posting 'distinguished', # Moderator actions happen after posting ] for col in unavailable_columns: if col in df.columns: # Check if this column has meaningful variation (not all zeros/nulls) non_null_values = df[col].dropna() if len(non_null_values) > 0: if pd.api.types.is_numeric_dtype(df[col]): if (non_null_values != 0).any(): issues.append(f"Column '{col}' contains post-publication information") else: if (non_null_values != '').any(): issues.append(f"Column '{col}' contains post-publication information") return issues def _find_near_duplicates(self, splits: Dict[str, pd.DataFrame]) -> List[Dict[str, Any]]: """ Find near-duplicate content across different splits using text similarity. Args: splits: Dictionary of split name to DataFrame Returns: List of near-duplicate pairs with similarity scores """ logger.info("Finding near-duplicate content across splits") near_duplicates = [] # Extract text content from each split split_texts = {} for split_name, df in splits.items(): texts = [] content_columns = ['clean_title', 'title', 'selftext'] available_columns = [col for col in content_columns if col in df.columns] for idx, row in df.iterrows(): combined_text = "" for col in available_columns: if pd.notna(row.get(col)): combined_text += " " + str(row[col]).strip() if combined_text.strip(): texts.append({ 'text': combined_text.strip().lower(), 'index': idx, 'split': split_name }) split_texts[split_name] = texts logger.info(f"Extracted {len(texts)} text samples from {split_name}") # Compare texts across different splits split_names = list(splits.keys()) for i, split1 in enumerate(split_names): for split2 in split_names[i+1:]: logger.info(f"Comparing {split1} vs {split2} for near-duplicates") # Sample comparison for performance (compare first 1000 from each) texts1 = split_texts[split1][:1000] texts2 = split_texts[split2][:1000] for text1_info in texts1: for text2_info in texts2: similarity = self._calculate_text_similarity( text1_info['text'], text2_info['text'] ) if similarity >= self.similarity_threshold: near_duplicates.append({ 'split1': split1, 'split2': split2, 'index1': text1_info['index'], 'index2': text2_info['index'], 'similarity': similarity, 'text1_preview': text1_info['text'][:100], 'text2_preview': text2_info['text'][:100] }) logger.info(f"Found {len(near_duplicates)} near-duplicate pairs") return near_duplicates def _calculate_text_similarity(self, text1: str, text2: str) -> float: """ Calculate similarity between two text strings using Jaccard similarity. Args: text1: First text string text2: Second text string Returns: Similarity score between 0 and 1 """ # Simple tokenization and cleaning def tokenize(text): # Remove punctuation and split into words cleaned = re.sub(r'[^\w\s]', ' ', text.lower()) return set(cleaned.split()) tokens1 = tokenize(text1) tokens2 = tokenize(text2) if not tokens1 and not tokens2: return 1.0 if not tokens1 or not tokens2: return 0.0 # Jaccard similarity intersection = len(tokens1.intersection(tokens2)) union = len(tokens1.union(tokens2)) return intersection / union if union > 0 else 0.0 def _calculate_leakage_score(self, duplicate_content: Dict[str, List[str]], temporal_issues: List[Dict[str, Any]], metadata_leakage: Dict[str, List[str]], near_duplicates: List[Dict[str, Any]], splits: Dict[str, pd.DataFrame]) -> float: """Calculate overall data leakage score (0-1, where 1 is maximum leakage)""" total_records = sum(len(df) for df in splits.values()) if total_records == 0: return 0.0 # Duplicate content score duplicate_score = len(duplicate_content) / total_records # Temporal issues score high_severity_temporal = sum(1 for issue in temporal_issues if issue.get('severity') == 'high') temporal_score = high_severity_temporal / len(splits) # Metadata leakage score total_metadata_issues = sum(len(issues) for issues in metadata_leakage.values()) metadata_score = total_metadata_issues / (len(splits) * 10) # Normalize by expected max issues # Near-duplicates score near_duplicate_score = len(near_duplicates) / total_records # Weighted combination leakage_score = ( 0.3 * duplicate_score + 0.3 * temporal_score + 0.2 * metadata_score + 0.2 * near_duplicate_score ) return min(1.0, leakage_score) def _generate_mitigation_strategies(self, duplicate_content: Dict[str, List[str]], temporal_issues: List[Dict[str, Any]], metadata_leakage: Dict[str, List[str]], near_duplicates: List[Dict[str, Any]]) -> List[str]: """Generate mitigation strategies based on detected leakage issues""" strategies = [] # Duplicate content mitigation if duplicate_content: strategies.append( f"Remove {len(duplicate_content)} exact duplicate content items across splits" ) strategies.append( "Implement content deduplication pipeline before train/validation/test splitting" ) # Temporal consistency mitigation high_severity_temporal = [issue for issue in temporal_issues if issue.get('severity') == 'high'] if high_severity_temporal: strategies.append( "Implement strict temporal splitting to ensure training data predates validation/test data" ) strategies.append( "Add temporal buffer period between training and evaluation data" ) # Metadata leakage mitigation if metadata_leakage: strategies.append( "Remove or mask metadata columns that contain post-publication information" ) strategies.append( "Validate that all features would be available at prediction time" ) # Specific column recommendations all_issues = [] for issues in metadata_leakage.values(): all_issues.extend(issues) if any('score' in issue for issue in all_issues): strategies.append("Consider removing or capping engagement metrics (score, comments)") if any('removed' in issue or 'deleted' in issue for issue in all_issues): strategies.append("Remove moderation-related columns (removed, deleted, locked)") # Near-duplicates mitigation if near_duplicates: strategies.append( f"Review and potentially remove {len(near_duplicates)} near-duplicate pairs" ) strategies.append( f"Implement fuzzy deduplication with similarity threshold < {self.similarity_threshold}" ) # General recommendations if not strategies: strategies.append("No significant data leakage detected - dataset appears well-structured") else: strategies.append("Implement automated leakage detection in data preprocessing pipeline") strategies.append("Document data collection and splitting methodology") return strategies def create_leakage_detector(**kwargs) -> DataLeakageDetector: """ Factory function to create a data leakage detector. Args: **kwargs: Configuration parameters Returns: Configured DataLeakageDetector instance """ return DataLeakageDetector(**kwargs)