#!/usr/bin/env python3 """ Complete Pipeline Execution - Tasks 1-3 This script executes the complete data processing pipeline from dataset loading through multimodal EDA, ensuring proper data volumes and comprehensive analysis. """ import sys import os from pathlib import Path import pandas as pd from dotenv import load_dotenv # Add src to path for imports sys.path.append(str(Path(__file__).parent.parent)) # Load environment variables load_dotenv() def load_full_dataset(): """Load the complete Fakeddit dataset with proper sampling.""" print("="*80) print("TASK 1: DATASET LOADING AND PREPROCESSING") print("="*80) # Get dataset paths from environment - use multimodal_only_samples base_dir = os.getenv('FAKEDDIT_BASE_DIR', '../Projects') train_path = f"{base_dir}/multimodal_only_samples_train.tsv" validation_path = f"{base_dir}/multimodal_only_samples_validate.tsv" test_path = f"{base_dir}/multimodal_only_samples_test_public.tsv" print(f"Loading from:") print(f" Train: {train_path}") print(f" Validation: {validation_path}") print(f" Test: {test_path}") # Create output directory output_dir = Path("analysis_results") output_dir.mkdir(exist_ok=True) datasets = {} # Load each split with proper sampling for analysis for split_name, file_path in [ ('train', train_path), ('validation', validation_path), ('test', test_path) ]: if file_path and Path(file_path).exists(): print(f"\nLoading {split_name} data...") # Load a substantial sample for meaningful analysis # Use 10,000 records per split for comprehensive analysis try: df = pd.read_csv(file_path, sep='\t', nrows=10000) print(f" Loaded {len(df)} records from {split_name}") # Basic preprocessing following dataset guidelines # Remove Unnamed columns as per dataset instructions unnamed_cols = [col for col in df.columns if col.startswith('Unnamed')] if unnamed_cols: df = df.drop(columns=unnamed_cols) print(f" Removed {len(unnamed_cols)} Unnamed columns") # Use clean_title as per dataset instructions if 'clean_title' not in df.columns and 'title' in df.columns: df['clean_title'] = df['title'].str.strip() # Ensure we have multimodal samples (both text and image) if 'hasImage' in df.columns: before_count = len(df) df = df[df['hasImage'] == True] print(f" Filtered to multimodal samples: {len(df)} (removed {before_count - len(df)} text-only)") # Ensure we have the required columns required_cols = ['clean_title', '2_way_label'] missing_cols = [col for col in required_cols if col not in df.columns] if missing_cols: print(f" Warning: Missing columns {missing_cols}") # Create missing columns with defaults if 'clean_title' in missing_cols and 'title' in df.columns: df['clean_title'] = df['title'] if '2_way_label' in missing_cols and '6_way_label' in df.columns: # Map 6-way to 2-way labels df['2_way_label'] = (df['6_way_label'] > 0).astype(int) # Add split identifier df['split'] = split_name # Save processed data output_file = output_dir / f"{split_name}_clean.parquet" df.to_parquet(output_file, index=False) print(f" Saved to {output_file}") datasets[split_name] = df except Exception as e: print(f" Error loading {split_name}: {e}") continue else: print(f" {split_name} file not found: {file_path}") # Generate summary report total_records = sum(len(df) for df in datasets.values()) print(f"\nDataset loading completed!") print(f" Total records loaded: {total_records:,}") for split_name, df in datasets.items(): if '2_way_label' in df.columns: true_count = (df['2_way_label'] == 0).sum() false_count = (df['2_way_label'] == 1).sum() print(f" {split_name.capitalize()}: {len(df):,} records ({true_count} true, {false_count} false)") return datasets def run_leakage_detection(): """Run leakage detection on the loaded dataset.""" print("\n" + "="*80) print("TASK 2: DATA LEAKAGE DETECTION") print("="*80) try: # Set output directory for leakage detection os.environ['OUTPUT_DIR'] = './analysis_results' from data.run_leakage_detection import main as run_leakage_main return run_leakage_main() except Exception as e: print(f"Error running leakage detection: {e}") return False def run_leakage_mitigation(): """Run leakage mitigation on the dataset.""" print("\n" + "="*80) print("TASK 2.1: DATA LEAKAGE MITIGATION") print("="*80) try: # Set output directory for leakage mitigation os.environ['OUTPUT_DIR'] = './analysis_results' from data.run_leakage_mitigation import main as run_mitigation_main return run_mitigation_main() except Exception as e: print(f"Error running leakage mitigation: {e}") return False def run_multimodal_eda(): """Run comprehensive multimodal EDA.""" print("\n" + "="*80) print("TASK 3: COMPREHENSIVE MULTIMODAL EDA") print("="*80) try: # Set output directory for multimodal EDA os.environ['OUTPUT_DIR'] = './analysis_results' from data.run_multimodal_eda import main as run_eda_main return run_eda_main() except Exception as e: print(f"Error running multimodal EDA: {e}") return False def main(): """Execute the complete pipeline.""" print("Starting Complete Data Processing Pipeline") print(" Tasks 1-3: Dataset Loading → Leakage Detection → Mitigation → Multimodal EDA") success_count = 0 total_tasks = 4 # Task 1: Dataset Loading try: datasets = load_full_dataset() if datasets: success_count += 1 print("Task 1: Dataset Loading - COMPLETED") else: print("Error: Task 1: Dataset Loading - FAILED") return False except Exception as e: print(f"Error: Task 1: Dataset Loading - FAILED: {e}") return False # Task 2: Leakage Detection try: if run_leakage_detection(): success_count += 1 print("Task 2: Leakage Detection - COMPLETED") else: print("Error: Task 2: Leakage Detection - FAILED") except Exception as e: print(f"Error: Task 2: Leakage Detection - FAILED: {e}") # Task 2.1: Leakage Mitigation try: if run_leakage_mitigation(): success_count += 1 print("Task 2.1: Leakage Mitigation - COMPLETED") else: print("Error: Task 2.1: Leakage Mitigation - FAILED") except Exception as e: print(f"Error: Task 2.1: Leakage Mitigation - FAILED: {e}") # Task 3: Multimodal EDA try: if run_multimodal_eda(): success_count += 1 print("Task 3: Multimodal EDA - COMPLETED") else: print("Error: Task 3: Multimodal EDA - FAILED") except Exception as e: print(f"Error: Task 3: Multimodal EDA - FAILED: {e}") # Final summary print("\n" + "="*80) print("PIPELINE EXECUTION SUMMARY") print("="*80) print(f"Completed: {success_count}/{total_tasks} tasks") if success_count == total_tasks: print("All tasks completed successfully!") print("Ready for comprehensive multimodal analysis and dashboard deployment") return True else: print(f"Warning: {total_tasks - success_count} task(s) had issues") print("Check logs above for details") return False if __name__ == "__main__": success = main() sys.exit(0 if success else 1)