""" Data Quality Management Module This module handles data quality validation, cleaning, and standardization for the Fakeddit dataset preprocessing pipeline. Requirements addressed: 1.4, 1.5 """ import pandas as pd import numpy as np import logging from typing import Dict, List, Optional, Tuple, Any from dataclasses import dataclass from datetime import datetime logger = logging.getLogger(__name__) @dataclass class QualityReport: """Data quality assessment report""" total_records: int missing_values: Dict[str, int] duplicate_records: int data_types: Dict[str, str] outliers: Dict[str, int] quality_score: float issues: List[str] timestamp: datetime class DataQualityManager: """ Manages data quality validation, cleaning, and standardization. This class implements comprehensive data quality checks and cleaning procedures while preserving data integrity for pattern analysis. """ def __init__(self, preserve_original: bool = True): self.preserve_original = preserve_original self.quality_thresholds = { 'missing_value_threshold': 0.5, # 50% missing values threshold 'duplicate_threshold': 0.1, # 10% duplicates threshold 'outlier_threshold': 0.05 # 5% outliers threshold } def validate_data_integrity(self, df: pd.DataFrame) -> QualityReport: """ Perform comprehensive data quality validation. Args: df: DataFrame to validate Returns: QualityReport with validation results Requirements: 1.4, 1.5 """ logger.info("Performing data integrity validation") issues = [] # Check missing values missing_values = df.isnull().sum().to_dict() missing_percentages = (df.isnull().sum() / len(df)).to_dict() for col, pct in missing_percentages.items(): if pct > self.quality_thresholds['missing_value_threshold']: issues.append(f"Column '{col}' has {pct:.2%} missing values") # Check for duplicates duplicate_count = df.duplicated().sum() duplicate_pct = duplicate_count / len(df) if duplicate_pct > self.quality_thresholds['duplicate_threshold']: issues.append(f"Dataset has {duplicate_pct:.2%} duplicate records") # Check data types data_types = df.dtypes.astype(str).to_dict() # Detect outliers in numeric columns outliers = {} numeric_cols = df.select_dtypes(include=[np.number]).columns for col in numeric_cols: if col in df.columns: Q1 = df[col].quantile(0.25) Q3 = df[col].quantile(0.75) IQR = Q3 - Q1 lower_bound = Q1 - 1.5 * IQR upper_bound = Q3 + 1.5 * IQR outlier_count = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum() outliers[col] = outlier_count outlier_pct = outlier_count / len(df) if outlier_pct > self.quality_thresholds['outlier_threshold']: issues.append(f"Column '{col}' has {outlier_pct:.2%} outliers") # Calculate overall quality score quality_score = self._calculate_quality_score(df, missing_percentages, duplicate_pct, outliers) report = QualityReport( total_records=len(df), missing_values=missing_values, duplicate_records=duplicate_count, data_types=data_types, outliers=outliers, quality_score=quality_score, issues=issues, timestamp=datetime.now() ) logger.info(f"Data quality validation completed. Quality score: {quality_score:.2f}") return report def _calculate_quality_score(self, df: pd.DataFrame, missing_percentages: Dict[str, float], duplicate_pct: float, outliers: Dict[str, int]) -> float: """Calculate overall data quality score (0-1)""" # Completeness score (1 - average missing percentage) avg_missing = np.mean(list(missing_percentages.values())) completeness_score = 1 - avg_missing # Uniqueness score (1 - duplicate percentage) uniqueness_score = 1 - duplicate_pct # Consistency score (based on outliers) total_outliers = sum(outliers.values()) outlier_pct = total_outliers / len(df) if len(df) > 0 else 0 consistency_score = 1 - outlier_pct # Weighted average quality_score = ( 0.4 * completeness_score + 0.3 * uniqueness_score + 0.3 * consistency_score ) return max(0, min(1, quality_score)) def handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame: """ Handle missing values with appropriate strategies. Args: df: DataFrame with missing values Returns: DataFrame with handled missing values Requirements: 1.4, 1.5 """ logger.info("Handling missing values") if self.preserve_original: df = df.copy() # Strategy for text columns text_columns = ['clean_title', 'title', 'selftext'] for col in text_columns: if col in df.columns: # Fill with empty string for text analysis df[col] = df[col].fillna('') # Strategy for numeric columns numeric_columns = ['score', 'num_comments', 'created_utc'] for col in numeric_columns: if col in df.columns: if col in ['score', 'num_comments']: # Fill with 0 for engagement metrics df[col] = df[col].fillna(0) elif col == 'created_utc': # Fill with median timestamp median_time = df[col].median() df[col] = df[col].fillna(median_time) # Strategy for categorical columns categorical_columns = ['subreddit', 'domain', 'author'] for col in categorical_columns: if col in df.columns: # Fill with 'unknown' category df[col] = df[col].fillna('unknown') logger.info("Missing value handling completed") return df def detect_duplicates(self, df: pd.DataFrame, subset: Optional[List[str]] = None) -> List[int]: """ Detect duplicate records. Args: df: DataFrame to check for duplicates subset: Columns to consider for duplicate detection Returns: List of duplicate record indices Requirements: 1.4, 1.5 """ logger.info("Detecting duplicate records") if subset is None: # Use key columns for duplicate detection subset = ['clean_title', 'created_utc', 'author'] subset = [col for col in subset if col in df.columns] if not subset: logger.warning("No suitable columns found for duplicate detection") return [] # Find duplicates duplicate_mask = df.duplicated(subset=subset, keep='first') duplicate_indices = df[duplicate_mask].index.tolist() logger.info(f"Found {len(duplicate_indices)} duplicate records") return duplicate_indices def standardize_formats(self, df: pd.DataFrame) -> pd.DataFrame: """ Standardize data formats for consistent analysis. Args: df: DataFrame to standardize Returns: DataFrame with standardized formats Requirements: 1.4, 1.5 """ logger.info("Standardizing data formats") if self.preserve_original: df = df.copy() # Standardize text columns text_columns = ['clean_title', 'title', 'selftext'] for col in text_columns: if col in df.columns: # Convert to string and strip whitespace df[col] = df[col].astype(str).str.strip() # Convert to lowercase for consistency df[col] = df[col].str.lower() # Remove extra whitespace df[col] = df[col].str.replace(r'\s+', ' ', regex=True) # Standardize timestamp columns if 'created_utc' in df.columns: # Convert to datetime df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s', errors='coerce') # Extract additional time features df['hour'] = df['created_utc'].dt.hour df['day_of_week'] = df['created_utc'].dt.dayofweek df['month'] = df['created_utc'].dt.month df['year'] = df['created_utc'].dt.year # Standardize categorical columns categorical_columns = ['subreddit', 'domain', 'author'] for col in categorical_columns: if col in df.columns: # Convert to lowercase and strip df[col] = df[col].astype(str).str.lower().str.strip() # Standardize numeric columns numeric_columns = ['score', 'num_comments'] for col in numeric_columns: if col in df.columns: # Ensure numeric type df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0) logger.info("Data format standardization completed") return df def generate_quality_metrics(self, df: pd.DataFrame) -> Dict[str, Any]: """ Generate comprehensive quality metrics. Args: df: DataFrame to analyze Returns: Dictionary with quality metrics Requirements: 1.4, 1.5 """ logger.info("Generating quality metrics") metrics = { 'total_records': len(df), 'total_columns': len(df.columns), 'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024, 'completeness': {}, 'uniqueness': {}, 'consistency': {} } # Completeness metrics for col in df.columns: non_null_count = df[col].count() completeness_pct = non_null_count / len(df) metrics['completeness'][col] = completeness_pct # Uniqueness metrics for col in df.columns: unique_count = df[col].nunique() uniqueness_pct = unique_count / len(df) if len(df) > 0 else 0 metrics['uniqueness'][col] = uniqueness_pct # Consistency metrics (for numeric columns) numeric_cols = df.select_dtypes(include=[np.number]).columns for col in numeric_cols: if col in df.columns and df[col].count() > 0: cv = df[col].std() / df[col].mean() if df[col].mean() != 0 else 0 metrics['consistency'][col] = 1 / (1 + cv) # Inverse coefficient of variation logger.info("Quality metrics generation completed") return metrics def clean_dataset(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, QualityReport]: """ Perform comprehensive dataset cleaning. Args: df: Raw dataset Returns: Tuple of (cleaned_dataframe, quality_report) Requirements: 1.4, 1.5 """ logger.info("Starting comprehensive dataset cleaning") # Initial quality assessment initial_report = self.validate_data_integrity(df) # Apply cleaning steps cleaned_df = df.copy() if self.preserve_original else df # 1. Handle missing values cleaned_df = self.handle_missing_values(cleaned_df) # 2. Remove duplicates duplicate_indices = self.detect_duplicates(cleaned_df) if duplicate_indices: cleaned_df = cleaned_df.drop(duplicate_indices).reset_index(drop=True) logger.info(f"Removed {len(duplicate_indices)} duplicate records") # 3. Standardize formats cleaned_df = self.standardize_formats(cleaned_df) # Final quality assessment final_report = self.validate_data_integrity(cleaned_df) logger.info(f"Dataset cleaning completed. Quality improved from {initial_report.quality_score:.2f} to {final_report.quality_score:.2f}") return cleaned_df, final_report