""" Data Leakage Mitigation Runner This script applies comprehensive data leakage mitigation strategies to create clean dataset splits and validates the effectiveness of the mitigation. Requirements addressed: 1.4, 1.5 """ import logging import sys from pathlib import Path from typing import Dict, Any import argparse import os from dotenv import load_dotenv import json # Load environment variables load_dotenv() # Add src to path for imports sys.path.append(str(Path(__file__).parent.parent)) from data.dataset_loader import create_dataset_loader from data.leakage_detection import create_leakage_detector from data.leakage_mitigation import create_leakage_mitigator from data.utils import MemoryManager, save_processed_data # Configure logging logging.basicConfig( level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', handlers=[ logging.FileHandler('leakage_mitigation.log'), logging.StreamHandler(sys.stdout) ] ) logger = logging.getLogger(__name__) def main(): """Main leakage mitigation function""" parser = argparse.ArgumentParser(description='Apply data leakage mitigation to Fakeddit dataset') parser.add_argument('--output-dir', type=str, default=os.getenv('OUTPUT_DIR', './clean_dataset_output'), help='Output directory for clean dataset') parser.add_argument('--similarity-threshold', type=float, default=0.8, help='Similarity threshold for near-duplicate removal') parser.add_argument('--temporal-buffer-days', type=int, default=7, help='Buffer days for temporal splitting') parser.add_argument('--memory-limit', type=float, default=float(os.getenv('MEMORY_LIMIT_GB', '8.0')), help='Memory limit in GB') parser.add_argument('--sample-size', type=int, default=10000, help='Sample size per split for processing (0 for full dataset)') parser.add_argument('--validate-mitigation', action='store_true', help='Run leakage detection on cleaned data to validate mitigation') args = parser.parse_args() logger.info("Starting data leakage mitigation process") logger.info(f"Output directory: {args.output_dir}") logger.info(f"Similarity threshold: {args.similarity_threshold}") logger.info(f"Temporal buffer: {args.temporal_buffer_days} days") logger.info(f"Sample size: {args.sample_size}") logger.info(f"Validate mitigation: {args.validate_mitigation}") # Initialize components memory_manager = MemoryManager(memory_limit_gb=args.memory_limit) dataset_loader = create_dataset_loader() leakage_detector = create_leakage_detector( similarity_threshold=args.similarity_threshold, temporal_buffer_days=args.temporal_buffer_days ) leakage_mitigator = create_leakage_mitigator( similarity_threshold=args.similarity_threshold, temporal_buffer_days=args.temporal_buffer_days, preserve_original=True ) # Create output directory output_dir = Path(args.output_dir) output_dir.mkdir(parents=True, exist_ok=True) try: with memory_manager.memory_monitor("Data leakage mitigation"): # Step 1: Load original dataset logger.info("Step 1: Loading original dataset splits") splits_data = {} split_names = ['train', 'validation', 'test'] for split_name in split_names: try: logger.info(f"Loading {split_name} split") split_df = dataset_loader.load_multimodal_samples(split_name) # Sample data if requested for performance if args.sample_size > 0 and len(split_df) > args.sample_size: split_df = split_df.sample(n=args.sample_size, random_state=42) logger.info(f"Sampled {args.sample_size} records from {split_name}") splits_data[split_name] = split_df logger.info(f"Loaded {len(split_df)} records from {split_name}") except FileNotFoundError: logger.warning(f"Split {split_name} not found, skipping") continue except Exception as e: logger.error(f"Error loading {split_name}: {str(e)}") continue if len(splits_data) < 2: logger.error("Need at least 2 splits for mitigation") return # Ensure we have required splits required_splits = ['train', 'validation', 'test'] import pandas as pd for split_name in required_splits: if split_name not in splits_data: splits_data[split_name] = pd.DataFrame() logger.warning(f"Using empty DataFrame for missing {split_name} split") # Step 2: Run initial leakage detection logger.info("Step 2: Running initial leakage detection") initial_report = leakage_detector.detect_data_leakage( train_df=splits_data['train'], validation_df=splits_data['validation'], test_df=splits_data['test'] ) logger.info(f"Initial leakage score: {initial_report.leakage_score:.4f}") # Step 3: Apply mitigation strategies logger.info("Step 3: Applying leakage mitigation strategies") clean_splits, mitigation_report = leakage_mitigator.mitigate_data_leakage( train_df=splits_data['train'], validation_df=splits_data['validation'], test_df=splits_data['test'], leakage_report=initial_report ) # Step 4: Save clean datasets logger.info("Step 4: Saving clean datasets") save_clean_datasets(clean_splits, output_dir) # Step 5: Validate mitigation effectiveness final_report = None if args.validate_mitigation: logger.info("Step 5: Validating mitigation effectiveness") final_report = leakage_detector.detect_data_leakage( train_df=clean_splits['train'], validation_df=clean_splits['validation'], test_df=clean_splits['test'] ) mitigation_report.final_leakage_score = final_report.leakage_score logger.info(f"Final leakage score: {final_report.leakage_score:.4f}") # Step 6: Generate comprehensive reports logger.info("Step 6: Generating mitigation reports") save_mitigation_report(mitigation_report, initial_report, final_report, output_dir) generate_mitigation_summary(mitigation_report, initial_report, final_report, output_dir) # Log results log_mitigation_results(mitigation_report, initial_report, final_report) logger.info("Data leakage mitigation completed successfully") return True except Exception as e: logger.error(f"Error during leakage mitigation: {str(e)}") return False def save_clean_datasets(clean_splits: Dict[str, Any], output_dir: Path): """Save clean datasets to files""" for split_name, df in clean_splits.items(): if len(df) > 0: # Save as parquet for efficiency output_path = output_dir / f"{split_name}_clean.parquet" success = save_processed_data(df, output_path, format='parquet') if success: logger.info(f"Saved clean {split_name} dataset: {len(df)} records to {output_path}") else: # Fallback to CSV csv_path = output_dir / f"{split_name}_clean.csv" df.to_csv(csv_path, index=False) logger.info(f"Saved clean {split_name} dataset as CSV: {len(df)} records to {csv_path}") def save_mitigation_report(mitigation_report, initial_report, final_report, output_dir: Path): """Save detailed mitigation report""" report_path = output_dir / "mitigation_report.txt" with open(report_path, 'w', encoding='utf-8') as f: f.write("Data Leakage Mitigation Report\n") f.write("==============================\n\n") f.write(f"Mitigation Timestamp: {mitigation_report.timestamp}\n\n") # Original vs cleaned sizes f.write("Dataset Size Changes:\n") for split in mitigation_report.original_sizes: original = mitigation_report.original_sizes[split] cleaned = mitigation_report.cleaned_sizes[split] reduction = original - cleaned reduction_pct = (reduction / original * 100) if original > 0 else 0 f.write(f" {split.capitalize()}:\n") f.write(f" Original: {original:,} records\n") f.write(f" Cleaned: {cleaned:,} records\n") f.write(f" Removed: {reduction:,} records ({reduction_pct:.1f}%)\n") f.write("\n") # Mitigation actions f.write("Mitigation Actions Performed:\n") f.write(f" Exact duplicates removed: {mitigation_report.removed_duplicates}\n") f.write(f" Near-duplicates removed: {mitigation_report.removed_near_duplicates}\n") f.write(f" Temporal splitting applied: {mitigation_report.temporal_split_applied}\n") f.write(f" Metadata columns removed: {len(mitigation_report.removed_metadata_columns)}\n") if mitigation_report.removed_metadata_columns: f.write(f" Columns: {', '.join(mitigation_report.removed_metadata_columns)}\n") f.write("\n") # Strategies applied f.write("Mitigation Strategies Applied:\n") for i, strategy in enumerate(mitigation_report.mitigation_strategies_applied, 1): f.write(f" {i}. {strategy}\n") f.write("\n") # Leakage score comparison if initial_report and final_report: f.write("Leakage Score Improvement:\n") f.write(f" Initial score: {initial_report.leakage_score:.4f}\n") f.write(f" Final score: {final_report.leakage_score:.4f}\n") improvement = initial_report.leakage_score - final_report.leakage_score improvement_pct = (improvement / initial_report.leakage_score * 100) if initial_report.leakage_score > 0 else 0 f.write(f" Improvement: {improvement:.4f} ({improvement_pct:.1f}%)\n") if final_report.leakage_score < 0.1: f.write(f" Status: LOW RISK - Mitigation successful\n") elif final_report.leakage_score < 0.3: f.write(f" Status: MEDIUM RISK - Partial mitigation\n") else: f.write(f" Status: HIGH RISK - Additional mitigation needed\n") f.write("\n") # Remaining issues (if any) if final_report: f.write("Remaining Issues After Mitigation:\n") f.write(f" Exact duplicates: {len(final_report.duplicate_content)}\n") f.write(f" Temporal issues: {len(final_report.temporal_inconsistencies)}\n") f.write(f" Metadata leakage: {len(final_report.metadata_leakage)} splits\n") f.write(f" Near-duplicates: {len(final_report.near_duplicate_pairs)} pairs\n") logger.info(f"Mitigation report saved to {report_path}") def generate_mitigation_summary(mitigation_report, initial_report, final_report, output_dir: Path): """Generate executive summary of mitigation results""" summary_path = output_dir / "mitigation_summary.json" summary = { "mitigation_timestamp": mitigation_report.timestamp.isoformat(), "original_sizes": mitigation_report.original_sizes, "cleaned_sizes": mitigation_report.cleaned_sizes, "records_removed": { "exact_duplicates": mitigation_report.removed_duplicates, "near_duplicates": mitigation_report.removed_near_duplicates, "total_reduction": sum(mitigation_report.original_sizes.values()) - sum(mitigation_report.cleaned_sizes.values()) }, "mitigation_actions": { "temporal_splitting_applied": mitigation_report.temporal_split_applied, "metadata_columns_removed": len(mitigation_report.removed_metadata_columns), "strategies_applied": len(mitigation_report.mitigation_strategies_applied) }, "leakage_scores": { "initial": initial_report.leakage_score if initial_report else None, "final": final_report.leakage_score if final_report else None, "improvement": (initial_report.leakage_score - final_report.leakage_score) if (initial_report and final_report) else None }, "final_risk_level": "low" if (final_report and final_report.leakage_score < 0.1) else "medium" if (final_report and final_report.leakage_score < 0.3) else "high", "mitigation_success": (final_report.leakage_score < initial_report.leakage_score) if (initial_report and final_report) else None } with open(summary_path, 'w') as f: json.dump(summary, f, indent=2) logger.info(f"Mitigation summary saved to {summary_path}") def log_mitigation_results(mitigation_report, initial_report, final_report): """Log key mitigation results to console""" logger.info("=== MITIGATION RESULTS SUMMARY ===") # Size changes total_original = sum(mitigation_report.original_sizes.values()) total_cleaned = sum(mitigation_report.cleaned_sizes.values()) total_removed = total_original - total_cleaned removal_pct = (total_removed / total_original * 100) if total_original > 0 else 0 logger.info(f"Dataset size: {total_original:,} → {total_cleaned:,} ({removal_pct:.1f}% reduction)") logger.info(f"Removed: {mitigation_report.removed_duplicates} exact + {mitigation_report.removed_near_duplicates} near duplicates") logger.info(f"Temporal splitting: {'Applied' if mitigation_report.temporal_split_applied else 'Not applied'}") logger.info(f"Metadata columns removed: {len(mitigation_report.removed_metadata_columns)}") # Leakage improvement if initial_report and final_report: improvement = initial_report.leakage_score - final_report.leakage_score improvement_pct = (improvement / initial_report.leakage_score * 100) if initial_report.leakage_score > 0 else 0 logger.info(f"Leakage score: {initial_report.leakage_score:.4f} → {final_report.leakage_score:.4f}") logger.info(f"Improvement: {improvement:.4f} ({improvement_pct:.1f}%)") if final_report.leakage_score < 0.1: logger.info("MITIGATION SUCCESSFUL - Low risk achieved") elif final_report.leakage_score < 0.3: logger.info("PARTIAL MITIGATION - Medium risk remaining") else: logger.info("MITIGATION INSUFFICIENT - High risk remains") logger.info("=== END MITIGATION SUMMARY ===") if __name__ == "__main__": main()