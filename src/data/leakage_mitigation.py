""" Data Leakage Mitigation Module This module implements comprehensive data leakage mitigation strategies based on detection results, creating clean dataset splits free from information leakage. Requirements addressed: 1.4, 1.5 """ import pandas as pd import numpy as np import logging from typing import Dict, List, Optional, Tuple, Set, Any from dataclasses import dataclass from datetime import datetime, timedelta from pathlib import Path import hashlib import re from collections import defaultdict logger = logging.getLogger(__name__) @dataclass class MitigationReport: """Data leakage mitigation report""" original_sizes: Dict[str, int] cleaned_sizes: Dict[str, int] removed_duplicates: int removed_near_duplicates: int removed_temporal_issues: int removed_metadata_columns: List[str] temporal_split_applied: bool mitigation_strategies_applied: List[str] final_leakage_score: Optional[float] timestamp: datetime class DataLeakageMitigator: """ Implements comprehensive data leakage mitigation strategies. This class applies various mitigation techniques to create clean dataset splits free from information leakage, based on detection results and best practices. """ def __init__(self, similarity_threshold: float = 0.8, temporal_buffer_days: int = 7, preserve_original: bool = True): """ Initialize data leakage mitigator. Args: similarity_threshold: Threshold for near-duplicate removal temporal_buffer_days: Buffer days for temporal splitting preserve_original: Whether to preserve original data """ self.similarity_threshold = similarity_threshold self.temporal_buffer_days = temporal_buffer_days self.preserve_original = preserve_original # Problematic metadata columns to remove/mask self.problematic_columns = [ 'removed', 'deleted', 'locked', 'archived', # Post-publication moderation 'gilded', 'distinguished', # Post-publication awards/actions 'stickied', 'contest_mode', # Post-publication modifications 'quarantine', 'over_18' # Post-publication classifications ] # Engagement metrics that may contain post-hoc information self.engagement_columns = [ 'score', 'num_comments', 'upvote_ratio', 'ups', 'downs', 'total_awards_received' ] def mitigate_data_leakage(self, train_df: pd.DataFrame, validation_df: pd.DataFrame, test_df: pd.DataFrame, leakage_report: Optional[Any] = None) -> Tuple[Dict[str, pd.DataFrame], MitigationReport]: """ Apply comprehensive data leakage mitigation strategies. Args: train_df: Training dataset validation_df: Validation dataset test_df: Test dataset leakage_report: Optional leakage detection report for targeted mitigation Returns: Tuple of (cleaned_splits, mitigation_report) Requirements: 1.4, 1.5 """ logger.info("Starting comprehensive data leakage mitigation") # Record original sizes original_sizes = { 'train': len(train_df), 'validation': len(validation_df), 'test': len(test_df) } # Copy data if preserving original if self.preserve_original: train_clean = train_df.copy() val_clean = validation_df.copy() test_clean = test_df.copy() else: train_clean = train_df val_clean = validation_df test_clean = test_df splits = { 'train': train_clean, 'validation': val_clean, 'test': test_clean } mitigation_strategies = [] removed_duplicates = 0 removed_near_duplicates = 0 removed_temporal_issues = 0 removed_columns = [] # 1. Remove exact duplicates logger.info("Step 1: Removing exact duplicate content") splits, dup_count = self._remove_exact_duplicates(splits) removed_duplicates = dup_count if dup_count > 0: mitigation_strategies.append(f"Removed {dup_count} exact duplicate content items") # 2. Remove near-duplicates logger.info("Step 2: Removing near-duplicate content") splits, near_dup_count = self._remove_near_duplicates(splits) removed_near_duplicates = near_dup_count if near_dup_count > 0: mitigation_strategies.append(f"Removed {near_dup_count} near-duplicate content pairs") # 3. Apply temporal splitting logger.info("Step 3: Applying temporal splitting") splits, temporal_applied = self._apply_temporal_splitting(splits) if temporal_applied: mitigation_strategies.append("Applied strict temporal splitting with buffer period") # 4. Remove problematic metadata logger.info("Step 4: Removing problematic metadata columns") splits, removed_cols = self._remove_problematic_metadata(splits) removed_columns = removed_cols if removed_cols: mitigation_strategies.append(f"Removed {len(removed_cols)} problematic metadata columns") # 5. Cap engagement metrics logger.info("Step 5: Capping engagement metrics") splits = self._cap_engagement_metrics(splits) mitigation_strategies.append("Applied engagement metric capping to reduce post-hoc bias") # 6. Validate prediction-time availability logger.info("Step 6: Validating prediction-time feature availability") splits = self._validate_prediction_time_features(splits) mitigation_strategies.append("Validated all features are available at prediction time") # Record final sizes cleaned_sizes = { 'train': len(splits['train']), 'validation': len(splits['validation']), 'test': len(splits['test']) } # Create mitigation report report = MitigationReport( original_sizes=original_sizes, cleaned_sizes=cleaned_sizes, removed_duplicates=removed_duplicates, removed_near_duplicates=removed_near_duplicates, removed_temporal_issues=removed_temporal_issues, removed_metadata_columns=removed_columns, temporal_split_applied=temporal_applied, mitigation_strategies_applied=mitigation_strategies, final_leakage_score=None, # Will be set by validation timestamp=datetime.now() ) logger.info("Data leakage mitigation completed") return splits, report def _remove_exact_duplicates(self, splits: Dict[str, pd.DataFrame]) -> Tuple[Dict[str, pd.DataFrame], int]: """Remove exact duplicate content across splits""" logger.info("Identifying and removing exact duplicates") # Create content hashes for all splits content_hashes = {} hash_to_split = {} for split_name, df in splits.items(): content_columns = ['clean_title', 'title', 'selftext'] available_columns = [col for col in content_columns if col in df.columns] if not available_columns: logger.warning(f"No content columns found in {split_name}") continue split_hashes = [] for idx, row in df.iterrows(): combined_content = "" for col in available_columns: if pd.notna(row.get(col)): combined_content += str(row[col]).strip().lower() if combined_content: content_hash = hashlib.md5(combined_content.encode()).hexdigest() split_hashes.append(content_hash) # Track which split this hash first appeared in if content_hash not in hash_to_split: hash_to_split[content_hash] = split_name else: split_hashes.append(None) content_hashes[split_name] = split_hashes # Identify duplicates to remove (keep first occurrence) removed_count = 0 cleaned_splits = {} for split_name, df in splits.items(): if split_name not in content_hashes: cleaned_splits[split_name] = df continue # Create mask for rows to keep keep_mask = [] hashes = content_hashes[split_name] for i, content_hash in enumerate(hashes): if content_hash is None: keep_mask.append(True) # Keep rows without content elif hash_to_split[content_hash] == split_name: keep_mask.append(True) # Keep first occurrence else: keep_mask.append(False) # Remove duplicate removed_count += 1 # Apply mask cleaned_df = df[keep_mask].reset_index(drop=True) cleaned_splits[split_name] = cleaned_df logger.info(f"Removed {len(df) - len(cleaned_df)} duplicates from {split_name}") logger.info(f"Total exact duplicates removed: {removed_count}") return cleaned_splits, removed_count def _remove_near_duplicates(self, splits: Dict[str, pd.DataFrame]) -> Tuple[Dict[str, pd.DataFrame], int]: """Remove near-duplicate content across splits""" logger.info("Identifying and removing near-duplicates") # Extract text content from each split split_texts = {} for split_name, df in splits.items(): texts = [] content_columns = ['clean_title', 'title', 'selftext'] available_columns = [col for col in content_columns if col in df.columns] for idx, row in df.iterrows(): combined_text = "" for col in available_columns: if pd.notna(row.get(col)): combined_text += " " + str(row[col]).strip() texts.append(combined_text.strip().lower() if combined_text.strip() else "") split_texts[split_name] = texts # Find near-duplicates across splits duplicate_indices = set() split_names = list(splits.keys()) for i, split1 in enumerate(split_names): for split2 in split_names[i+1:]: texts1 = split_texts[split1] texts2 = split_texts[split2] # Compare texts (sample for performance) sample_size = min(1000, len(texts1), len(texts2)) for idx1 in range(min(sample_size, len(texts1))): for idx2 in range(min(sample_size, len(texts2))): if texts1[idx1] and texts2[idx2]: similarity = self._calculate_text_similarity(texts1[idx1], texts2[idx2]) if similarity >= self.similarity_threshold: # Remove from later split (preserve training data priority) if split1 == 'train': duplicate_indices.add((split2, idx2)) elif split2 == 'train': duplicate_indices.add((split1, idx1)) elif split1 == 'validation': duplicate_indices.add((split2, idx2)) else: duplicate_indices.add((split1, idx1)) # Remove identified near-duplicates cleaned_splits = {} removed_count = 0 for split_name, df in splits.items(): # Get indices to remove for this split indices_to_remove = {idx for split, idx in duplicate_indices if split == split_name} if indices_to_remove: # Create mask for rows to keep keep_mask = [i not in indices_to_remove for i in range(len(df))] cleaned_df = df[keep_mask].reset_index(drop=True) removed_count += len(indices_to_remove) logger.info(f"Removed {len(indices_to_remove)} near-duplicates from {split_name}") else: cleaned_df = df cleaned_splits[split_name] = cleaned_df logger.info(f"Total near-duplicates removed: {removed_count}") return cleaned_splits, removed_count def _calculate_text_similarity(self, text1: str, text2: str) -> float: """Calculate Jaccard similarity between two texts""" def tokenize(text): cleaned = re.sub(r'[^\w\s]', ' ', text.lower()) return set(cleaned.split()) tokens1 = tokenize(text1) tokens2 = tokenize(text2) if not tokens1 and not tokens2: return 1.0 if not tokens1 or not tokens2: return 0.0 intersection = len(tokens1.intersection(tokens2)) union = len(tokens1.union(tokens2)) return intersection / union if union > 0 else 0.0 def _apply_temporal_splitting(self, splits: Dict[str, pd.DataFrame]) -> Tuple[Dict[str, pd.DataFrame], bool]: """Apply strict temporal splitting with buffer periods""" logger.info("Applying temporal splitting") temporal_columns = ['created_utc', 'timestamp'] temporal_col = None # Find available temporal column for split_name, df in splits.items(): for col in temporal_columns: if col in df.columns: temporal_col = col break if temporal_col: break if not temporal_col: logger.warning("No temporal column found for temporal splitting") return splits, False # Convert to datetime and sort cleaned_splits = {} all_timestamps = [] for split_name, df in splits.items(): df_copy = df.copy() # Convert to datetime if not pd.api.types.is_datetime64_any_dtype(df_copy[temporal_col]): if df_copy[temporal_col].dtype in ['int64', 'float64']: df_copy[temporal_col] = pd.to_datetime(df_copy[temporal_col], unit='s', errors='coerce') else: df_copy[temporal_col] = pd.to_datetime(df_copy[temporal_col], errors='coerce') # Remove rows with invalid timestamps df_copy = df_copy.dropna(subset=[temporal_col]) all_timestamps.extend(df_copy[temporal_col].tolist()) cleaned_splits[split_name] = df_copy if not all_timestamps: logger.warning("No valid timestamps found") return splits, False # Calculate split points with buffer all_timestamps.sort() total_samples = len(all_timestamps) # Use 70/15/15 split with buffers train_end_idx = int(0.70 * total_samples) val_end_idx = int(0.85 * total_samples) train_cutoff = all_timestamps[train_end_idx] val_start = train_cutoff + timedelta(days=self.temporal_buffer_days) val_cutoff = all_timestamps[val_end_idx] test_start = val_cutoff + timedelta(days=self.temporal_buffer_days) # Apply temporal filtering final_splits = {} # Training data: earliest to train_cutoff train_mask = cleaned_splits['train'][temporal_col] <= train_cutoff final_splits['train'] = cleaned_splits['train'][train_mask].reset_index(drop=True) # Validation data: val_start to val_cutoff val_mask = (cleaned_splits['validation'][temporal_col] >= val_start) & \ (cleaned_splits['validation'][temporal_col] <= val_cutoff) final_splits['validation'] = cleaned_splits['validation'][val_mask].reset_index(drop=True) # Test data: test_start onwards test_mask = cleaned_splits['test'][temporal_col] >= test_start final_splits['test'] = cleaned_splits['test'][test_mask].reset_index(drop=True) logger.info(f"Applied temporal splitting with {self.temporal_buffer_days}-day buffers") logger.info(f"Train: {len(final_splits['train'])}, Val: {len(final_splits['validation'])}, Test: {len(final_splits['test'])}") return final_splits, True def _remove_problematic_metadata(self, splits: Dict[str, pd.DataFrame]) -> Tuple[Dict[str, pd.DataFrame], List[str]]: """Remove metadata columns that contain post-publication information""" logger.info("Removing problematic metadata columns") removed_columns = [] cleaned_splits = {} for split_name, df in splits.items(): df_copy = df.copy() # Remove problematic columns if they exist cols_to_remove = [] for col in self.problematic_columns: if col in df_copy.columns: cols_to_remove.append(col) if cols_to_remove: df_copy = df_copy.drop(columns=cols_to_remove) removed_columns.extend(cols_to_remove) logger.info(f"Removed {len(cols_to_remove)} problematic columns from {split_name}: {cols_to_remove}") cleaned_splits[split_name] = df_copy # Remove duplicates from removed_columns list removed_columns = list(set(removed_columns)) return cleaned_splits, removed_columns def _cap_engagement_metrics(self, splits: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]: """Cap engagement metrics to reduce post-hoc bias""" logger.info("Capping engagement metrics") cleaned_splits = {} for split_name, df in splits.items(): df_copy = df.copy() for col in self.engagement_columns: if col in df_copy.columns: # Cap at 95th percentile to reduce extreme outliers cap_value = df_copy[col].quantile(0.95) original_max = df_copy[col].max() if cap_value < original_max: df_copy[col] = df_copy[col].clip(upper=cap_value) logger.info(f"Capped {col} in {split_name} at {cap_value} (was {original_max})") cleaned_splits[split_name] = df_copy return cleaned_splits def _validate_prediction_time_features(self, splits: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]: """Validate that all features would be available at prediction time""" logger.info("Validating prediction-time feature availability") # Features that should be available at prediction time allowed_features = [ 'id', 'clean_title', 'title', 'selftext', 'author', 'subreddit', 'domain', 'url', 'created_utc', 'hasImage', 'image_url', '2_way_label', '6_way_label', 'authenticity_label', 'category_label' ] cleaned_splits = {} for split_name, df in splits.items(): # Keep only allowed features (if they exist) available_allowed = [col for col in allowed_features if col in df.columns] # Also keep any derived features we created derived_features = [col for col in df.columns if col.startswith(('has_', 'is_', 'num_', 'avg_', 'total_'))] # Keep temporal features we derived temporal_features = [col for col in df.columns if col in ['hour', 'day_of_week', 'month', 'year']] final_columns = available_allowed + derived_features + temporal_features final_columns = list(set(final_columns)) # Remove duplicates # Filter dataframe to keep only valid columns df_filtered = df[final_columns] cleaned_splits[split_name] = df_filtered logger.info(f"Kept {len(final_columns)} prediction-time features in {split_name}") return cleaned_splits def create_leakage_mitigator(**kwargs) -> DataLeakageMitigator: """ Factory function to create a data leakage mitigator. Args: **kwargs: Configuration parameters Returns: Configured DataLeakageMitigator instance """ return DataLeakageMitigator(**kwargs)