""" Data Leakage Detection Runner This script runs comprehensive data leakage detection on the Fakeddit dataset splits and generates detailed reports with mitigation strategies. Requirements addressed: 1.4, 1.5 """ import logging import sys from pathlib import Path from typing import Dict, Any import argparse import os from dotenv import load_dotenv import json # Load environment variables load_dotenv() # Add src to path for imports sys.path.append(str(Path(__file__).parent.parent)) from data.dataset_loader import create_dataset_loader from data.leakage_detection import create_leakage_detector, LeakageReport from data.utils import MemoryManager # Configure logging logging.basicConfig( level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', handlers=[ logging.FileHandler('leakage_detection.log'), logging.StreamHandler(sys.stdout) ] ) logger = logging.getLogger(__name__) def main(): """Main leakage detection function""" parser = argparse.ArgumentParser(description='Run data leakage detection on Fakeddit dataset') parser.add_argument('--output-dir', type=str, default=os.getenv('OUTPUT_DIR', './leakage_analysis_output'), help='Output directory for leakage analysis results') parser.add_argument('--similarity-threshold', type=float, default=0.85, help='Similarity threshold for near-duplicate detection') parser.add_argument('--temporal-buffer-days', type=int, default=1, help='Buffer days for temporal consistency checks') parser.add_argument('--memory-limit', type=float, default=float(os.getenv('MEMORY_LIMIT_GB', '8.0')), help='Memory limit in GB') parser.add_argument('--sample-size', type=int, default=10000, help='Sample size per split for analysis (0 for full dataset)') args = parser.parse_args() logger.info("Starting data leakage detection analysis") logger.info(f"Output directory: {args.output_dir}") logger.info(f"Similarity threshold: {args.similarity_threshold}") logger.info(f"Temporal buffer: {args.temporal_buffer_days} days") logger.info(f"Sample size: {args.sample_size}") # Initialize components memory_manager = MemoryManager(memory_limit_gb=args.memory_limit) dataset_loader = create_dataset_loader() leakage_detector = create_leakage_detector( similarity_threshold=args.similarity_threshold, temporal_buffer_days=args.temporal_buffer_days ) # Create output directory output_dir = Path(args.output_dir) output_dir.mkdir(parents=True, exist_ok=True) try: with memory_manager.memory_monitor("Data leakage detection"): # Load dataset splits logger.info("Loading dataset splits for leakage analysis") splits_data = {} split_names = ['train', 'validation', 'test'] for split_name in split_names: try: logger.info(f"Loading {split_name} split") split_df = dataset_loader.load_multimodal_samples(split_name) # Sample data if requested for performance if args.sample_size > 0 and len(split_df) > args.sample_size: split_df = split_df.sample(n=args.sample_size, random_state=42) logger.info(f"Sampled {args.sample_size} records from {split_name}") splits_data[split_name] = split_df logger.info(f"Loaded {len(split_df)} records from {split_name}") except FileNotFoundError: logger.warning(f"Split {split_name} not found, skipping") continue except Exception as e: logger.error(f"Error loading {split_name}: {str(e)}") continue if len(splits_data) < 2: logger.error("Need at least 2 splits for leakage detection") return # Ensure we have the required splits required_splits = ['train', 'validation', 'test'] available_splits = list(splits_data.keys()) # Use available splits, create empty DataFrames for missing ones import pandas as pd for split_name in required_splits: if split_name not in splits_data: splits_data[split_name] = pd.DataFrame() logger.warning(f"Using empty DataFrame for missing {split_name} split") # Run leakage detection logger.info("Running comprehensive leakage detection") leakage_report = leakage_detector.detect_data_leakage( train_df=splits_data['train'], validation_df=splits_data['validation'], test_df=splits_data['test'] ) # Save detailed report save_leakage_report(leakage_report, output_dir) # Generate summary generate_leakage_summary(leakage_report, splits_data, output_dir) # Save mitigation recommendations save_mitigation_strategies(leakage_report, output_dir) # Log key findings log_key_findings(leakage_report) logger.info("Data leakage detection completed successfully") return True except Exception as e: logger.error(f"Error during leakage detection: {str(e)}") return False def save_leakage_report(report: LeakageReport, output_dir: Path): """Save detailed leakage detection report""" report_path = output_dir / "leakage_detection_report.txt" with open(report_path, 'w', encoding='utf-8') as f: f.write("Data Leakage Detection Report\n") f.write("============================\n\n") f.write(f"Analysis Timestamp: {report.timestamp}\n") f.write(f"Overall Leakage Score: {report.leakage_score:.4f} (0=no leakage, 1=maximum leakage)\n\n") # Records checked f.write("Records Analyzed:\n") for split, count in report.total_records_checked.items(): f.write(f" {split}: {count:,} records\n") f.write("\n") # Duplicate content f.write("Duplicate Content Detection:\n") if report.duplicate_content: f.write(f" Found {len(report.duplicate_content)} exact duplicates across splits\n") for content_hash, splits in list(report.duplicate_content.items())[:10]: # Show first 10 f.write(f" Content hash {content_hash[:8]}... appears in: {', '.join(splits)}\n") if len(report.duplicate_content) > 10: f.write(f" ... and {len(report.duplicate_content) - 10} more duplicates\n") else: f.write(" No exact duplicate content found across splits\n") f.write("\n") # Temporal consistency f.write("Temporal Consistency Analysis:\n") if report.temporal_inconsistencies: f.write(f" Found {len(report.temporal_inconsistencies)} temporal issues\n") for issue in report.temporal_inconsistencies: f.write(f" {issue['issue_type']}: {issue.get('splits', 'N/A')} - Severity: {issue.get('severity', 'unknown')}\n") if 'overlap_days' in issue: f.write(f" Temporal overlap: {issue['overlap_days']} days\n") if 'gap_days' in issue: f.write(f" Temporal gap: {issue['gap_days']} days\n") else: f.write(" No temporal consistency issues detected\n") f.write("\n") # Metadata leakage f.write("Metadata Leakage Detection:\n") if report.metadata_leakage: f.write(f" Found potential metadata leakage in {len(report.metadata_leakage)} splits\n") for split, issues in report.metadata_leakage.items(): f.write(f" {split}:\n") for issue in issues: f.write(f" - {issue}\n") else: f.write(" No metadata leakage detected\n") f.write("\n") # Near duplicates f.write("Near-Duplicate Content Detection:\n") if report.near_duplicate_pairs: f.write(f" Found {len(report.near_duplicate_pairs)} near-duplicate pairs\n") for pair in report.near_duplicate_pairs[:5]: # Show first 5 f.write(f" {pair['split1']} vs {pair['split2']}: {pair['similarity']:.3f} similarity\n") f.write(f" Preview 1: {pair['text1_preview']}...\n") f.write(f" Preview 2: {pair['text2_preview']}...\n") if len(report.near_duplicate_pairs) > 5: f.write(f" ... and {len(report.near_duplicate_pairs) - 5} more pairs\n") else: f.write(" No near-duplicate content found\n") f.write("\n") # Mitigation strategies f.write("Recommended Mitigation Strategies:\n") for i, strategy in enumerate(report.mitigation_strategies, 1): f.write(f" {i}. {strategy}\n") logger.info(f"Detailed leakage report saved to {report_path}") def save_mitigation_strategies(report: LeakageReport, output_dir: Path): """Save mitigation strategies as a separate actionable document""" strategies_path = output_dir / "mitigation_strategies.md" with open(strategies_path, 'w', encoding='utf-8') as f: f.write("# Data Leakage Mitigation Strategies\n\n") f.write(f"**Analysis Date:** {report.timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n") f.write(f"**Leakage Score:** {report.leakage_score:.4f}/1.0\n\n") if report.leakage_score < 0.1: f.write("## Status: Low Risk\n") f.write("The dataset shows minimal signs of data leakage.\n\n") elif report.leakage_score < 0.3: f.write("## Warning: Status: Medium Risk\n") f.write("Some potential leakage issues detected that should be addressed.\n\n") else: f.write("## ðŸš¨ Status: High Risk\n") f.write("Significant data leakage detected. Immediate action required.\n\n") f.write("## Recommended Actions\n\n") for i, strategy in enumerate(report.mitigation_strategies, 1): f.write(f"{i}. {strategy}\n") f.write("\n## Implementation Priority\n\n") # Prioritize based on detected issues if report.duplicate_content: f.write("### High Priority\n") f.write("- Remove exact duplicate content across splits\n") f.write("- Implement deduplication before data splitting\n\n") if any(issue.get('severity') == 'high' for issue in report.temporal_inconsistencies): f.write("### High Priority\n") f.write("- Fix temporal consistency issues\n") f.write("- Implement proper temporal splitting\n\n") if report.metadata_leakage: f.write("### Medium Priority\n") f.write("- Review and remove problematic metadata columns\n") f.write("- Validate feature availability at prediction time\n\n") if report.near_duplicate_pairs: f.write("### Medium Priority\n") f.write("- Review near-duplicate content pairs\n") f.write("- Consider fuzzy deduplication\n\n") f.write("## Validation Steps\n\n") f.write("1. Re-run leakage detection after implementing fixes\n") f.write("2. Verify leakage score improvement\n") f.write("3. Document changes in data preprocessing pipeline\n") f.write("4. Update data collection procedures if necessary\n") logger.info(f"Mitigation strategies saved to {strategies_path}") def generate_leakage_summary(report: LeakageReport, splits_data: Dict, output_dir: Path): """Generate executive summary of leakage detection results""" summary_path = output_dir / "leakage_summary.json" summary = { "analysis_timestamp": report.timestamp.isoformat(), "overall_leakage_score": report.leakage_score, "risk_level": "low" if report.leakage_score < 0.1 else "medium" if report.leakage_score < 0.3 else "high", "splits_analyzed": {split: len(df) for split, df in splits_data.items()}, "issues_detected": { "exact_duplicates": len(report.duplicate_content), "temporal_issues": len(report.temporal_inconsistencies), "metadata_leakage_splits": len(report.metadata_leakage), "near_duplicate_pairs": len(report.near_duplicate_pairs) }, "high_priority_issues": [], "mitigation_strategies_count": len(report.mitigation_strategies) } # Identify high priority issues if report.duplicate_content: summary["high_priority_issues"].append("exact_duplicates_across_splits") if any(issue.get('severity') == 'high' for issue in report.temporal_inconsistencies): summary["high_priority_issues"].append("temporal_consistency_violations") if report.metadata_leakage: summary["high_priority_issues"].append("metadata_leakage_detected") with open(summary_path, 'w') as f: json.dump(summary, f, indent=2) logger.info(f"Leakage summary saved to {summary_path}") def log_key_findings(report: LeakageReport): """Log key findings to console""" logger.info("=== KEY LEAKAGE DETECTION FINDINGS ===") logger.info(f"Overall Leakage Score: {report.leakage_score:.4f}") if report.leakage_score < 0.1: logger.info("LOW RISK: Minimal data leakage detected") elif report.leakage_score < 0.3: logger.info("Warning: MEDIUM RISK: Some leakage issues detected") else: logger.info("ðŸš¨ HIGH RISK: Significant data leakage detected") logger.info(f"Exact duplicates: {len(report.duplicate_content)}") logger.info(f"Temporal issues: {len(report.temporal_inconsistencies)}") logger.info(f"Metadata leakage: {len(report.metadata_leakage)} splits affected") logger.info(f"Near-duplicates: {len(report.near_duplicate_pairs)} pairs") if report.mitigation_strategies: logger.info("Top mitigation strategies:") for i, strategy in enumerate(report.mitigation_strategies[:3], 1): logger.info(f" {i}. {strategy}") logger.info("=== END FINDINGS ===") if __name__ == "__main__": main()