""" Main processing script for Fakeddit dataset loading and preprocessing. This script demonstrates the complete dataset loading and preprocessing pipeline for the multimodal fake news detection project. Requirements addressed: 1.1, 1.2, 1.4, 1.5 """ import logging import sys from pathlib import Path from typing import Dict, Any import argparse import os from dotenv import load_dotenv # Load environment variables load_dotenv() # Add src to path for imports sys.path.append(str(Path(__file__).parent)) from data.dataset_loader import create_dataset_loader, DatasetConfig from data.data_quality import DataQualityManager from data.utils import MemoryManager, create_data_summary, save_processed_data # Configure logging logging.basicConfig( level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', handlers=[ logging.FileHandler('dataset_processing.log'), logging.StreamHandler(sys.stdout) ] ) logger = logging.getLogger(__name__) def main(): """Main processing function""" parser = argparse.ArgumentParser(description='Process Fakeddit dataset') parser.add_argument('--output-dir', type=str, default=os.getenv('OUTPUT_DIR', './processed_data'), help='Output directory for processed data') parser.add_argument('--chunk-size', type=int, default=int(os.getenv('CHUNK_SIZE', '10000')), help='Chunk size for processing large files') parser.add_argument('--memory-limit', type=float, default=float(os.getenv('MEMORY_LIMIT_GB', '8.0')), help='Memory limit in GB') parser.add_argument('--splits', nargs='+', default=['train', 'validation', 'test'], help='Dataset splits to process') parser.add_argument('--demo-mode', action='store_true', help='Run in demo mode with limited data processing') args = parser.parse_args() logger.info("Starting Fakeddit dataset processing") logger.info(f"Base directory: {os.getenv('FAKEDDIT_BASE_DIR')}") logger.info(f"Output directory: {args.output_dir}") logger.info(f"Chunk size: {args.chunk_size}") logger.info(f"Memory limit: {args.memory_limit} GB") logger.info(f"Demo mode: {args.demo_mode}") # Initialize components memory_manager = MemoryManager(memory_limit_gb=args.memory_limit) data_quality_manager = DataQualityManager(preserve_original=True) # Create dataset loader (uses .env configuration) dataset_loader = create_dataset_loader( chunk_size=args.chunk_size, memory_limit_gb=args.memory_limit ) # Create output directory output_dir = Path(args.output_dir) output_dir.mkdir(parents=True, exist_ok=True) try: with memory_manager.memory_monitor("Complete dataset processing"): # Load complete dataset split_data, segmented_data = dataset_loader.load_complete_dataset() # Process each split for split_name, split_df in split_data.items(): logger.info(f"Processing {split_name} split") with memory_manager.memory_monitor(f"{split_name} split processing"): # Data quality assessment and cleaning cleaned_df, quality_report = data_quality_manager.clean_dataset(split_df) # Generate data summary data_summary = create_data_summary(cleaned_df) # Save processed data split_output_path = output_dir / f"{split_name}_processed.parquet" save_processed_data(cleaned_df, split_output_path) # Save quality report quality_output_path = output_dir / f"{split_name}_quality_report.txt" save_quality_report(quality_report, quality_output_path) # Save data summary summary_output_path = output_dir / f"{split_name}_summary.txt" save_data_summary(data_summary, summary_output_path) # Process category segmentation if split_name in segmented_data: category_data = segmented_data[split_name] for category, category_df in category_data.items(): if len(category_df) > 0: category_output_path = output_dir / f"{split_name}_{category.lower()}_processed.parquet" save_processed_data(category_df, category_output_path) logger.info(f"Saved {category} category data: {len(category_df)} samples") logger.info(f"Completed processing {split_name} split: {len(cleaned_df)} samples") # Generate overall processing summary generate_processing_summary(split_data, segmented_data, output_dir) logger.info("Dataset processing completed successfully") except Exception as e: logger.error(f"Error during dataset processing: {str(e)}") raise def save_quality_report(quality_report, output_path: Path): """Save data quality report to file""" with open(output_path, 'w') as f: f.write("Data Quality Report\n") f.write("==================\n\n") f.write(f"Timestamp: {quality_report.timestamp}\n") f.write(f"Total Records: {quality_report.total_records:,}\n") f.write(f"Quality Score: {quality_report.quality_score:.3f}\n") f.write(f"Duplicate Records: {quality_report.duplicate_records:,}\n\n") f.write("Missing Values by Column:\n") for col, count in quality_report.missing_values.items(): pct = (count / quality_report.total_records) * 100 f.write(f" {col}: {count:,} ({pct:.2f}%)\n") f.write("\nOutliers by Column:\n") for col, count in quality_report.outliers.items(): pct = (count / quality_report.total_records) * 100 f.write(f" {col}: {count:,} ({pct:.2f}%)\n") if quality_report.issues: f.write("\nIdentified Issues:\n") for issue in quality_report.issues: f.write(f" - {issue}\n") def save_data_summary(data_summary: Dict[str, Any], output_path: Path): """Save data summary to file""" with open(output_path, 'w') as f: f.write("Data Summary Report\n") f.write("==================\n\n") f.write(f"Dataset Shape: {data_summary['shape'][0]:,} rows Ã— {data_summary['shape'][1]} columns\n") f.write(f"Memory Usage: {data_summary['memory_usage']['total_mb']:.2f} MB\n\n") f.write("Column Information:\n") for col, info in data_summary['column_info'].items(): f.write(f" {col}:\n") f.write(f" Type: {info['dtype']}\n") f.write(f" Non-null: {info['non_null_count']:,} ({100-info['null_percentage']:.1f}%)\n") f.write(f" Unique: {info['unique_count']:,} ({info['unique_percentage']:.1f}%)\n") if data_summary['numeric_summary']: f.write("\nNumeric Column Statistics:\n") for col, stats in data_summary['numeric_summary'].items(): f.write(f" {col}:\n") f.write(f" Mean: {stats['mean']:.3f}\n") f.write(f" Median: {stats['median']:.3f}\n") f.write(f" Std: {stats['std']:.3f}\n") f.write(f" Range: [{stats['min']:.3f}, {stats['max']:.3f}]\n") if data_summary['categorical_summary']: f.write("\nCategorical Column Top Values:\n") for col, stats in data_summary['categorical_summary'].items(): f.write(f" {col}:\n") f.write(f" Most frequent: {stats['most_frequent']} ({stats['most_frequent_count']:,})\n") f.write(f" Top values: {list(stats['top_values'].keys())[:5]}\n") def generate_processing_summary(split_data: Dict, segmented_data: Dict, output_dir: Path): """Generate overall processing summary""" summary_path = output_dir / "processing_summary.txt" with open(summary_path, 'w') as f: f.write("Fakeddit Dataset Processing Summary\n") f.write("==================================\n\n") f.write("Dataset Splits:\n") total_samples = 0 for split_name, split_df in split_data.items(): f.write(f" {split_name}: {len(split_df):,} samples\n") total_samples += len(split_df) f.write(f"\nTotal Samples: {total_samples:,}\n\n") f.write("Category Distribution:\n") for split_name, categories in segmented_data.items(): f.write(f" {split_name}:\n") for category, category_df in categories.items(): pct = (len(category_df) / len(split_data[split_name])) * 100 f.write(f" {category}: {len(category_df):,} ({pct:.1f}%)\n") f.write(f"\nProcessed files saved to: {output_dir}\n") f.write("Files include:\n") f.write(" - *_processed.parquet: Main processed datasets\n") f.write(" - *_quality_report.txt: Data quality assessments\n") f.write(" - *_summary.txt: Data summary statistics\n") f.write(" - *_[category]_processed.parquet: Category-segmented data\n") if __name__ == "__main__": main()