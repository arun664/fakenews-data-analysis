""" Comprehensive Text Analysis Module for Multimodal Fake News Detection This module implements linguistic feature extraction and authenticity pattern detection for the Fakeddit dataset, focusing on identifying distinguishing textual characteristics between true and false content. Requirements addressed: 2.1, 2.2 (Linguistic Pattern Mining and Authenticity Detection) """ import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from typing import Dict, List, Tuple, Optional, Any import warnings from pathlib import Path import json from datetime import datetime import os import re from collections import Counter, defaultdict import string warnings.filterwarnings('ignore') # Try to import text processing libraries try: import nltk from nltk.corpus import stopwords from nltk.tokenize import word_tokenize, sent_tokenize from nltk.sentiment import SentimentIntensityAnalyzer NLTK_AVAILABLE = True # Download required NLTK data try: nltk.download('punkt', quiet=True) nltk.download('punkt_tab', quiet=True) nltk.download('stopwords', quiet=True) nltk.download('vader_lexicon', quiet=True) except: pass except ImportError: NLTK_AVAILABLE = False try: from textstat import flesch_reading_ease, flesch_kincaid_grade TEXTSTAT_AVAILABLE = True except ImportError: TEXTSTAT_AVAILABLE = False try: from wordcloud import WordCloud WORDCLOUD_AVAILABLE = True except ImportError: WORDCLOUD_AVAILABLE = False try: from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer from sklearn.decomposition import LatentDirichletAllocation from scipy import stats SKLEARN_AVAILABLE = True except ImportError: SKLEARN_AVAILABLE = False class TextAnalyzer: """ Comprehensive text analysis for linguistic pattern mining and authenticity detection. This class analyzes text content to identify distinguishing features between true and false content, extract linguistic patterns, and generate insights. """ def __init__(self, data_dir: str = "processed_data/text_data"): """Initialize the text analyzer with processed data directory.""" self.data_dir = Path(data_dir) self.output_dir = Path("analysis_results") self.viz_dir = Path("visualizations") # Create output directories self.output_dir.mkdir(parents=True, exist_ok=True) self.viz_dir.mkdir(parents=True, exist_ok=True) # Data containers self.text_data = None self.analysis_results = {} # Initialize text processing tools if NLTK_AVAILABLE: try: self.sentiment_analyzer = SentimentIntensityAnalyzer() self.stop_words = set(stopwords.words('english')) except: self.sentiment_analyzer = None self.stop_words = set() else: self.sentiment_analyzer = None self.stop_words = set() # Category mappings self.binary_mapping = {0: "True", 1: "False"} self.multiway_mapping = {0: "True", 1: "Satire", 2: "Misleading", 3: "False"} def load_processed_data(self) -> bool: """ Load processed text data from clean datasets. Returns: bool: True if data loaded successfully """ print("Loading processed text data...") try: # Load all splits datasets = [] splits = ['train', 'validation', 'test'] for split in splits: file_path = self.data_dir / f"{split}_clean.parquet" if file_path.exists(): df = pd.read_parquet(file_path) df['split'] = split datasets.append(df) print(f" Loaded {split}: {len(df)} records") if datasets: self.text_data = pd.concat(datasets, ignore_index=True) print(f"Combined text data: {len(self.text_data)} total records") # Validate required columns required_cols = ['clean_title', '2_way_label'] missing_cols = [col for col in required_cols if col not in self.text_data.columns] if missing_cols: print(f"Warning: Missing required columns: {missing_cols}") return False return True else: print("Error: No processed data files found") return False except Exception as e: print(f"Error loading processed data: {e}") return False def extract_linguistic_features(self) -> Dict[str, Any]: """ Extract comprehensive linguistic features from text data. Returns: Dict: Linguistic features analysis results """ print("Extracting linguistic features...") if self.text_data is None: raise ValueError("Data not loaded. Call load_processed_data() first.") features = { 'basic_statistics': {}, 'readability_metrics': {}, 'sentiment_analysis': {}, 'lexical_diversity': {}, 'syntactic_features': {}, 'category_comparisons': {} } # Focus on clean_title column for analysis text_col = 'clean_title' if text_col not in self.text_data.columns: print(f"Error: Column '{text_col}' not found") return features # Clean and prepare text data text_data = self.text_data[text_col].dropna().astype(str) labels = self.text_data.loc[text_data.index, '2_way_label'] print(f" Analyzing {len(text_data)} text samples...") # Basic text statistics features['basic_statistics'] = self._extract_basic_statistics(text_data, labels) # Readability metrics (if available) if TEXTSTAT_AVAILABLE: features['readability_metrics'] = self._extract_readability_metrics(text_data, labels) # Sentiment analysis (if available) if self.sentiment_analyzer: features['sentiment_analysis'] = self._extract_sentiment_features(text_data, labels) # Lexical diversity features['lexical_diversity'] = self._extract_lexical_diversity(text_data, labels) # Syntactic features features['syntactic_features'] = self._extract_syntactic_features(text_data, labels) # Category comparisons features['category_comparisons'] = self._compare_categories(text_data, labels) self.analysis_results['linguistic_features'] = features print("Linguistic feature extraction completed") return features def _extract_basic_statistics(self, text_data: pd.Series, labels: pd.Series) -> Dict: """Extract basic text statistics by category.""" print(" Extracting basic text statistics...") stats = {} # Overall statistics lengths = text_data.str.len() word_counts = text_data.str.split().str.len() stats['overall'] = { 'total_samples': len(text_data), 'avg_char_length': float(lengths.mean()), 'median_char_length': float(lengths.median()), 'std_char_length': float(lengths.std()), 'avg_word_count': float(word_counts.mean()), 'median_word_count': float(word_counts.median()), 'std_word_count': float(word_counts.std()) } # Statistics by category stats['by_category'] = {} for label_val in labels.unique(): if pd.notna(label_val): category_name = self.binary_mapping.get(label_val, f"Category_{label_val}") mask = labels == label_val cat_texts = text_data[mask] cat_lengths = cat_texts.str.len() cat_word_counts = cat_texts.str.split().str.len() stats['by_category'][category_name] = { 'sample_count': int(mask.sum()), 'avg_char_length': float(cat_lengths.mean()), 'median_char_length': float(cat_lengths.median()), 'std_char_length': float(cat_lengths.std()), 'avg_word_count': float(cat_word_counts.mean()), 'median_word_count': float(cat_word_counts.median()), 'std_word_count': float(cat_word_counts.std()) } return stats def _extract_readability_metrics(self, text_data: pd.Series, labels: pd.Series) -> Dict: """Extract readability metrics by category.""" print(" Extracting readability metrics...") readability = {'by_category': {}} # Sample for performance (readability calculation is slow) sample_size = min(500, len(text_data)) sample_indices = np.random.choice(len(text_data), sample_size, replace=False) for label_val in labels.unique(): if pd.notna(label_val): category_name = self.binary_mapping.get(label_val, f"Category_{label_val}") mask = labels == label_val # Get sample from this category cat_indices = text_data.index[mask] cat_sample_indices = [i for i in sample_indices if i in cat_indices] if len(cat_sample_indices) > 10: # Need minimum samples cat_texts = text_data.iloc[cat_sample_indices] flesch_scores = [] fk_grades = [] for text in cat_texts: try: if len(text.strip()) > 10: # Minimum text length flesch_scores.append(flesch_reading_ease(text)) fk_grades.append(flesch_kincaid_grade(text)) except: continue if flesch_scores: readability['by_category'][category_name] = { 'flesch_reading_ease': { 'mean': float(np.mean(flesch_scores)), 'std': float(np.std(flesch_scores)), 'median': float(np.median(flesch_scores)) }, 'flesch_kincaid_grade': { 'mean': float(np.mean(fk_grades)), 'std': float(np.std(fk_grades)), 'median': float(np.median(fk_grades)) }, 'sample_size': len(flesch_scores) } return readability def _extract_sentiment_features(self, text_data: pd.Series, labels: pd.Series) -> Dict: """Extract sentiment features by category.""" print(" Extracting sentiment features...") sentiment = {'by_category': {}} # Sample for performance sample_size = min(1000, len(text_data)) sample_indices = np.random.choice(len(text_data), sample_size, replace=False) for label_val in labels.unique(): if pd.notna(label_val): category_name = self.binary_mapping.get(label_val, f"Category_{label_val}") mask = labels == label_val # Get sample from this category cat_indices = text_data.index[mask] cat_sample_indices = [i for i in sample_indices if i in cat_indices] if len(cat_sample_indices) > 10: cat_texts = text_data.iloc[cat_sample_indices] sentiment_scores = { 'compound': [], 'positive': [], 'negative': [], 'neutral': [] } for text in cat_texts: try: scores = self.sentiment_analyzer.polarity_scores(text) sentiment_scores['compound'].append(scores['compound']) sentiment_scores['positive'].append(scores['pos']) sentiment_scores['negative'].append(scores['neg']) sentiment_scores['neutral'].append(scores['neu']) except: continue if sentiment_scores['compound']: sentiment['by_category'][category_name] = {} for score_type, scores in sentiment_scores.items(): sentiment['by_category'][category_name][score_type] = { 'mean': float(np.mean(scores)), 'std': float(np.std(scores)), 'median': float(np.median(scores)) } sentiment['by_category'][category_name]['sample_size'] = len(sentiment_scores['compound']) return sentiment def _extract_lexical_diversity(self, text_data: pd.Series, labels: pd.Series) -> Dict: """Extract lexical diversity metrics by category.""" print(" Extracting lexical diversity metrics...") diversity = {'by_category': {}} for label_val in labels.unique(): if pd.notna(label_val): category_name = self.binary_mapping.get(label_val, f"Category_{label_val}") mask = labels == label_val cat_texts = text_data[mask] # Sample for performance sample_size = min(300, len(cat_texts)) cat_sample = cat_texts.sample(n=sample_size, random_state=42) # Calculate lexical diversity metrics type_token_ratios = [] unique_word_ratios = [] avg_word_lengths = [] for text in cat_sample: try: # Simple tokenization (fallback if NLTK not available) try: if NLTK_AVAILABLE: words = word_tokenize(text.lower()) else: words = text.lower().split() except: # Fallback to simple split if NLTK fails words = text.lower().split() words = [w for w in words if w.isalpha() and w not in self.stop_words] if len(words) > 0: # Type-Token Ratio (lexical diversity) unique_words = set(words) ttr = len(unique_words) / len(words) type_token_ratios.append(ttr) # Unique word ratio unique_ratio = len(unique_words) / len(words) unique_word_ratios.append(unique_ratio) # Average word length avg_length = np.mean([len(w) for w in words]) avg_word_lengths.append(avg_length) except: continue if type_token_ratios: diversity['by_category'][category_name] = { 'type_token_ratio': { 'mean': float(np.mean(type_token_ratios)), 'std': float(np.std(type_token_ratios)), 'median': float(np.median(type_token_ratios)) }, 'unique_word_ratio': { 'mean': float(np.mean(unique_word_ratios)), 'std': float(np.std(unique_word_ratios)), 'median': float(np.median(unique_word_ratios)) }, 'avg_word_length': { 'mean': float(np.mean(avg_word_lengths)), 'std': float(np.std(avg_word_lengths)), 'median': float(np.median(avg_word_lengths)) }, 'sample_size': len(type_token_ratios) } return diversity def _extract_syntactic_features(self, text_data: pd.Series, labels: pd.Series) -> Dict: """Extract syntactic features by category.""" print(" Extracting syntactic features...") syntactic = {'by_category': {}} for label_val in labels.unique(): if pd.notna(label_val): category_name = self.binary_mapping.get(label_val, f"Category_{label_val}") mask = labels == label_val cat_texts = text_data[mask] # Sample for performance sample_size = min(200, len(cat_texts)) cat_sample = cat_texts.sample(n=sample_size, random_state=42) # Calculate syntactic features sentence_counts = [] avg_sentence_lengths = [] punctuation_ratios = [] capitalization_ratios = [] question_ratios = [] exclamation_ratios = [] for text in cat_sample: try: # Sentence analysis (simple fallback if NLTK not available) try: if NLTK_AVAILABLE: sentences = sent_tokenize(text) else: sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()] except: # Fallback to simple regex split if NLTK fails sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()] sentence_counts.append(len(sentences)) if sentences: sentence_lengths = [len(sent.split()) for sent in sentences] avg_sentence_lengths.append(np.mean(sentence_lengths)) else: avg_sentence_lengths.append(0) # Punctuation analysis punct_count = sum(1 for c in text if c in string.punctuation) punctuation_ratios.append(punct_count / len(text) if len(text) > 0 else 0) # Capitalization analysis cap_count = sum(1 for c in text if c.isupper()) capitalization_ratios.append(cap_count / len(text) if len(text) > 0 else 0) # Question and exclamation analysis question_count = text.count('?') exclamation_count = text.count('!') question_ratios.append(question_count / len(sentences) if sentences else 0) exclamation_ratios.append(exclamation_count / len(sentences) if sentences else 0) except: continue if sentence_counts: syntactic['by_category'][category_name] = { 'sentence_count': { 'mean': float(np.mean(sentence_counts)), 'std': float(np.std(sentence_counts)), 'median': float(np.median(sentence_counts)) }, 'avg_sentence_length': { 'mean': float(np.mean(avg_sentence_lengths)), 'std': float(np.std(avg_sentence_lengths)), 'median': float(np.median(avg_sentence_lengths)) }, 'punctuation_ratio': { 'mean': float(np.mean(punctuation_ratios)), 'std': float(np.std(punctuation_ratios)), 'median': float(np.median(punctuation_ratios)) }, 'capitalization_ratio': { 'mean': float(np.mean(capitalization_ratios)), 'std': float(np.std(capitalization_ratios)), 'median': float(np.median(capitalization_ratios)) }, 'question_ratio': { 'mean': float(np.mean(question_ratios)), 'std': float(np.std(question_ratios)), 'median': float(np.median(question_ratios)) }, 'exclamation_ratio': { 'mean': float(np.mean(exclamation_ratios)), 'std': float(np.std(exclamation_ratios)), 'median': float(np.median(exclamation_ratios)) }, 'sample_size': len(sentence_counts) } return syntactic def _compare_categories(self, text_data: pd.Series, labels: pd.Series) -> Dict: """Compare linguistic features between categories using statistical tests.""" print(" Performing statistical comparisons between categories...") comparisons = {} # Get data for each category true_mask = labels == 0 false_mask = labels == 1 if true_mask.sum() > 0 and false_mask.sum() > 0: true_texts = text_data[true_mask] false_texts = text_data[false_mask] # Compare text lengths true_lengths = true_texts.str.len() false_lengths = false_texts.str.len() # Statistical test for length differences try: if SKLEARN_AVAILABLE: length_stat, length_p = stats.mannwhitneyu(true_lengths, false_lengths, alternative='two-sided') comparisons['length_comparison'] = { 'statistic': float(length_stat), 'p_value': float(length_p), 'significant': length_p < 0.05, 'true_mean': float(true_lengths.mean()), 'false_mean': float(false_lengths.mean()), 'effect_size': float(abs(true_lengths.mean() - false_lengths.mean()) / np.sqrt((true_lengths.var() + false_lengths.var()) / 2)) } else: # Simple comparison without statistical test comparisons['length_comparison'] = { 'true_mean': float(true_lengths.mean()), 'false_mean': float(false_lengths.mean()), 'difference': float(false_lengths.mean() - true_lengths.mean()), 'relative_difference': float((false_lengths.mean() - true_lengths.mean()) / true_lengths.mean() * 100) } except: comparisons['length_comparison'] = {'error': 'Could not perform statistical test'} # Compare word counts true_word_counts = true_texts.str.split().str.len() false_word_counts = false_texts.str.split().str.len() try: if SKLEARN_AVAILABLE: word_stat, word_p = stats.mannwhitneyu(true_word_counts, false_word_counts, alternative='two-sided') comparisons['word_count_comparison'] = { 'statistic': float(word_stat), 'p_value': float(word_p), 'significant': word_p < 0.05, 'true_mean': float(true_word_counts.mean()), 'false_mean': float(false_word_counts.mean()), 'effect_size': float(abs(true_word_counts.mean() - false_word_counts.mean()) / np.sqrt((true_word_counts.var() + false_word_counts.var()) / 2)) } else: # Simple comparison without statistical test comparisons['word_count_comparison'] = { 'true_mean': float(true_word_counts.mean()), 'false_mean': float(false_word_counts.mean()), 'difference': float(false_word_counts.mean() - true_word_counts.mean()), 'relative_difference': float((false_word_counts.mean() - true_word_counts.mean()) / true_word_counts.mean() * 100) } except: comparisons['word_count_comparison'] = {'error': 'Could not perform statistical test'} return comparisons def detect_authenticity_patterns(self) -> Dict[str, Any]: """ Detect patterns that distinguish true from false content. Returns: Dict: Authenticity pattern analysis results """ print("Detecting authenticity patterns...") if self.text_data is None: raise ValueError("Data not loaded. Call load_processed_data() first.") patterns = { 'keyword_analysis': {}, 'linguistic_signatures': {}, 'distinguishing_features': {} } # Focus on clean_title column text_col = 'clean_title' text_data = self.text_data[text_col].dropna().astype(str) labels = self.text_data.loc[text_data.index, '2_way_label'] # Keyword analysis patterns['keyword_analysis'] = self._analyze_keywords_by_category(text_data, labels) # Linguistic signatures (if sklearn available) if SKLEARN_AVAILABLE: patterns['linguistic_signatures'] = self._extract_linguistic_signatures(text_data, labels) # Distinguishing features patterns['distinguishing_features'] = self._identify_distinguishing_features(text_data, labels) self.analysis_results['authenticity_patterns'] = patterns print("Authenticity pattern detection completed") return patterns def _analyze_keywords_by_category(self, text_data: pd.Series, labels: pd.Series) -> Dict: """Analyze most frequent keywords by category.""" print(" Analyzing keywords by category...") keyword_analysis = {'by_category': {}} for label_val in labels.unique(): if pd.notna(label_val): category_name = self.binary_mapping.get(label_val, f"Category_{label_val}") mask = labels == label_val cat_texts = text_data[mask] # Combine all texts for this category combined_text = ' '.join(cat_texts.astype(str)) # Simple tokenization and cleaning try: if NLTK_AVAILABLE: words = word_tokenize(combined_text.lower()) else: words = combined_text.lower().split() except: # Fallback to simple split if NLTK fails words = combined_text.lower().split() words = [w for w in words if w.isalpha() and w not in self.stop_words and len(w) > 2] # Count frequencies word_freq = Counter(words) top_words = word_freq.most_common(20) keyword_analysis['by_category'][category_name] = { 'top_keywords': [(word, int(count)) for word, count in top_words], 'total_words': len(words), 'unique_words': len(set(words)), 'vocabulary_size': len(word_freq) } return keyword_analysis def _extract_linguistic_signatures(self, text_data: pd.Series, labels: pd.Series) -> Dict: """Extract linguistic signatures that characterize each category.""" print(" Extracting linguistic signatures...") signatures = {} try: # Prepare texts by category true_texts = text_data[labels == 0].tolist() false_texts = text_data[labels == 1].tolist() if len(true_texts) > 0 and len(false_texts) > 0: # Create TF-IDF vectors vectorizer = TfidfVectorizer( max_features=500, stop_words='english', ngram_range=(1, 2), min_df=2, max_df=0.95 ) # Combine and fit all_texts = true_texts + false_texts tfidf_matrix = vectorizer.fit_transform(all_texts) feature_names = vectorizer.get_feature_names_out() # Split back into categories true_tfidf = tfidf_matrix[:len(true_texts)] false_tfidf = tfidf_matrix[len(true_texts):] # Calculate mean TF-IDF scores for each category true_means = np.array(true_tfidf.mean(axis=0)).flatten() false_means = np.array(false_tfidf.mean(axis=0)).flatten() # Find distinctive features true_distinctive = [] false_distinctive = [] for i, feature in enumerate(feature_names): true_score = true_means[i] false_score = false_means[i] # Features more characteristic of true content if true_score > false_score * 1.5 and true_score > 0.01: true_distinctive.append((feature, float(true_score), float(false_score))) # Features more characteristic of false content elif false_score > true_score * 1.5 and false_score > 0.01: false_distinctive.append((feature, float(false_score), float(true_score))) # Sort by distinctiveness true_distinctive.sort(key=lambda x: x[1] - x[2], reverse=True) false_distinctive.sort(key=lambda x: x[1] - x[2], reverse=True) signatures['tfidf_signatures'] = { 'true_distinctive_features': true_distinctive[:15], 'false_distinctive_features': false_distinctive[:15] } except Exception as e: signatures['tfidf_signatures'] = {'error': str(e)} return signatures def _identify_distinguishing_features(self, text_data: pd.Series, labels: pd.Series) -> Dict: """Identify the most distinguishing features between true and false content.""" print(" Identifying distinguishing features...") features = {} # Get basic statistics for comparison true_mask = labels == 0 false_mask = labels == 1 if true_mask.sum() > 0 and false_mask.sum() > 0: true_texts = text_data[true_mask] false_texts = text_data[false_mask] # Length-based features true_lengths = true_texts.str.len() false_lengths = false_texts.str.len() features['length_patterns'] = { 'true_content': { 'avg_length': float(true_lengths.mean()), 'median_length': float(true_lengths.median()), 'length_variance': float(true_lengths.var()) }, 'false_content': { 'avg_length': float(false_lengths.mean()), 'median_length': float(false_lengths.median()), 'length_variance': float(false_lengths.var()) }, 'length_difference': float(false_lengths.mean() - true_lengths.mean()), 'relative_difference': float((false_lengths.mean() - true_lengths.mean()) / true_lengths.mean() * 100) } # Punctuation patterns true_punct = true_texts.apply(lambda x: sum(1 for c in x if c in string.punctuation) / len(x) if len(x) > 0 else 0) false_punct = false_texts.apply(lambda x: sum(1 for c in x if c in string.punctuation) / len(x) if len(x) > 0 else 0) features['punctuation_patterns'] = { 'true_content': { 'avg_punctuation_ratio': float(true_punct.mean()), 'median_punctuation_ratio': float(true_punct.median()) }, 'false_content': { 'avg_punctuation_ratio': float(false_punct.mean()), 'median_punctuation_ratio': float(false_punct.median()) }, 'punctuation_difference': float(false_punct.mean() - true_punct.mean()) } # Capitalization patterns true_caps = true_texts.apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0) false_caps = false_texts.apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0) features['capitalization_patterns'] = { 'true_content': { 'avg_capitalization_ratio': float(true_caps.mean()), 'median_capitalization_ratio': float(true_caps.median()) }, 'false_content': { 'avg_capitalization_ratio': float(false_caps.mean()), 'median_capitalization_ratio': float(false_caps.median()) }, 'capitalization_difference': float(false_caps.mean() - true_caps.mean()) } return features def create_text_visualizations(self) -> bool: """ Create comprehensive visualizations for text analysis results. Returns: bool: True if visualizations created successfully """ print("Creating text analysis visualizations...") if not self.analysis_results: print("Error: No analysis results available. Run analysis first.") return False try: # Set up plotting style plt.style.use('default') sns.set_palette("husl") # 1. Basic statistics comparison self._create_basic_stats_visualization() # 2. Readability comparison (if available) if TEXTSTAT_AVAILABLE and 'readability_metrics' in self.analysis_results.get('linguistic_features', {}): self._create_readability_visualization() # 3. Sentiment analysis visualization (if available) if self.sentiment_analyzer and 'sentiment_analysis' in self.analysis_results.get('linguistic_features', {}): self._create_sentiment_visualization() # 4. Keyword analysis visualization self._create_keyword_visualization() # 5. Distinguishing features visualization self._create_distinguishing_features_visualization() print("Text analysis visualizations created successfully") return True except Exception as e: print(f"Error creating visualizations: {e}") return False def _create_basic_stats_visualization(self): """Create basic statistics comparison visualization.""" if 'linguistic_features' not in self.analysis_results: return stats = self.analysis_results['linguistic_features']['basic_statistics'] if 'by_category' not in stats: return fig, axes = plt.subplots(2, 2, figsize=(15, 10)) fig.suptitle('Text Statistics Comparison by Category', fontsize=16, fontweight='bold') categories = list(stats['by_category'].keys()) # Character length comparison char_lengths = [stats['by_category'][cat]['avg_char_length'] for cat in categories] axes[0, 0].bar(categories, char_lengths, color=['skyblue', 'lightcoral']) axes[0, 0].set_title('Average Character Length') axes[0, 0].set_ylabel('Characters') # Word count comparison word_counts = [stats['by_category'][cat]['avg_word_count'] for cat in categories] axes[0, 1].bar(categories, word_counts, color=['lightgreen', 'orange']) axes[0, 1].set_title('Average Word Count') axes[0, 1].set_ylabel('Words') # Character length standard deviation char_stds = [stats['by_category'][cat]['std_char_length'] for cat in categories] axes[1, 0].bar(categories, char_stds, color=['gold', 'pink']) axes[1, 0].set_title('Character Length Variability') axes[1, 0].set_ylabel('Standard Deviation') # Word count standard deviation word_stds = [stats['by_category'][cat]['std_word_count'] for cat in categories] axes[1, 1].bar(categories, word_stds, color=['lightblue', 'lightcoral']) axes[1, 1].set_title('Word Count Variability') axes[1, 1].set_ylabel('Standard Deviation') plt.tight_layout() plt.savefig(self.viz_dir / 'text_basic_statistics.png', dpi=300, bbox_inches='tight') plt.close() def _create_readability_visualization(self): """Create readability metrics visualization.""" readability = self.analysis_results['linguistic_features']['readability_metrics'] if 'by_category' not in readability or not readability['by_category']: return fig, axes = plt.subplots(1, 2, figsize=(15, 6)) fig.suptitle('Readability Metrics Comparison by Category', fontsize=16, fontweight='bold') categories = list(readability['by_category'].keys()) # Flesch Reading Ease flesch_scores = [readability['by_category'][cat]['flesch_reading_ease']['mean'] for cat in categories] axes[0].bar(categories, flesch_scores, color=['skyblue', 'lightcoral']) axes[0].set_title('Flesch Reading Ease') axes[0].set_ylabel('Score (Higher = Easier)') # Flesch-Kincaid Grade fk_grades = [readability['by_category'][cat]['flesch_kincaid_grade']['mean'] for cat in categories] axes[1].bar(categories, fk_grades, color=['lightgreen', 'orange']) axes[1].set_title('Flesch-Kincaid Grade Level') axes[1].set_ylabel('Grade Level') plt.tight_layout() plt.savefig(self.viz_dir / 'text_readability_metrics.png', dpi=300, bbox_inches='tight') plt.close() def _create_sentiment_visualization(self): """Create sentiment analysis visualization.""" sentiment = self.analysis_results['linguistic_features']['sentiment_analysis'] if 'by_category' not in sentiment or not sentiment['by_category']: return fig, axes = plt.subplots(2, 2, figsize=(15, 10)) fig.suptitle('Sentiment Analysis Comparison by Category', fontsize=16, fontweight='bold') categories = list(sentiment['by_category'].keys()) # Compound sentiment compound_scores = [sentiment['by_category'][cat]['compound']['mean'] for cat in categories] axes[0, 0].bar(categories, compound_scores, color=['skyblue', 'lightcoral']) axes[0, 0].set_title('Compound Sentiment Score') axes[0, 0].set_ylabel('Score (-1 to 1)') axes[0, 0].axhline(y=0, color='black', linestyle='--', alpha=0.5) # Positive sentiment positive_scores = [sentiment['by_category'][cat]['positive']['mean'] for cat in categories] axes[0, 1].bar(categories, positive_scores, color=['lightgreen', 'orange']) axes[0, 1].set_title('Positive Sentiment') axes[0, 1].set_ylabel('Score (0 to 1)') # Negative sentiment negative_scores = [sentiment['by_category'][cat]['negative']['mean'] for cat in categories] axes[1, 0].bar(categories, negative_scores, color=['gold', 'pink']) axes[1, 0].set_title('Negative Sentiment') axes[1, 0].set_ylabel('Score (0 to 1)') # Neutral sentiment neutral_scores = [sentiment['by_category'][cat]['neutral']['mean'] for cat in categories] axes[1, 1].bar(categories, neutral_scores, color=['lightblue', 'lightcoral']) axes[1, 1].set_title('Neutral Sentiment') axes[1, 1].set_ylabel('Score (0 to 1)') plt.tight_layout() plt.savefig(self.viz_dir / 'text_sentiment_analysis.png', dpi=300, bbox_inches='tight') plt.close() def _create_keyword_visualization(self): """Create keyword analysis visualization.""" if 'authenticity_patterns' not in self.analysis_results: return keywords = self.analysis_results['authenticity_patterns']['keyword_analysis'] if 'by_category' not in keywords: return fig, axes = plt.subplots(1, 2, figsize=(20, 8)) fig.suptitle('Top Keywords by Category', fontsize=16, fontweight='bold') for i, (category, data) in enumerate(keywords['by_category'].items()): if i < 2: # Only plot first two categories top_words = data['top_keywords'][:15] # Top 15 words words = [item[0] for item in top_words] counts = [item[1] for item in top_words] axes[i].barh(words[::-1], counts[::-1]) # Reverse for better readability axes[i].set_title(f'Top Keywords - {category}') axes[i].set_xlabel('Frequency') plt.tight_layout() plt.savefig(self.viz_dir / 'text_keyword_analysis.png', dpi=300, bbox_inches='tight') plt.close() def _create_distinguishing_features_visualization(self): """Create distinguishing features visualization.""" if 'authenticity_patterns' not in self.analysis_results: return features = self.analysis_results['authenticity_patterns']['distinguishing_features'] if not features: return fig, axes = plt.subplots(1, 3, figsize=(18, 6)) fig.suptitle('Distinguishing Features Between True and False Content', fontsize=16, fontweight='bold') # Length patterns if 'length_patterns' in features: length_data = features['length_patterns'] categories = ['True', 'False'] lengths = [length_data['true_content']['avg_length'], length_data['false_content']['avg_length']] axes[0].bar(categories, lengths, color=['skyblue', 'lightcoral']) axes[0].set_title('Average Text Length') axes[0].set_ylabel('Characters') # Punctuation patterns if 'punctuation_patterns' in features: punct_data = features['punctuation_patterns'] punct_ratios = [punct_data['true_content']['avg_punctuation_ratio'], punct_data['false_content']['avg_punctuation_ratio']] axes[1].bar(categories, punct_ratios, color=['lightgreen', 'orange']) axes[1].set_title('Punctuation Usage') axes[1].set_ylabel('Punctuation Ratio') # Capitalization patterns if 'capitalization_patterns' in features: cap_data = features['capitalization_patterns'] cap_ratios = [cap_data['true_content']['avg_capitalization_ratio'], cap_data['false_content']['avg_capitalization_ratio']] axes[2].bar(categories, cap_ratios, color=['gold', 'pink']) axes[2].set_title('Capitalization Usage') axes[2].set_ylabel('Capitalization Ratio') plt.tight_layout() plt.savefig(self.viz_dir / 'text_distinguishing_features.png', dpi=300, bbox_inches='tight') plt.close() def save_analysis_results(self) -> bool: """ Save analysis results to files. Returns: bool: True if results saved successfully """ print("Saving text analysis results...") if not self.analysis_results: print("Error: No analysis results to save") return False try: # Save comprehensive results as JSON results_file = self.output_dir / 'text_analysis_results.json' with open(results_file, 'w') as f: json.dump(self.analysis_results, f, indent=2, default=str) # Save summary report self._create_summary_report() print(f"Analysis results saved to {self.output_dir}/") return True except Exception as e: print(f"Error saving results: {e}") return False def _create_summary_report(self): """Create a summary report in markdown format.""" report_file = self.output_dir / 'TEXT_ANALYSIS_REPORT.md' with open(report_file, 'w') as f: f.write("# Text Analysis Report\n\n") f.write(f"**Analysis Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n") # Dataset overview if self.text_data is not None: f.write("## Dataset Overview\n\n") f.write(f"- **Total Records:** {len(self.text_data):,}\n") f.write(f"- **True Content:** {(self.text_data['2_way_label'] == 0).sum():,}\n") f.write(f"- **False Content:** {(self.text_data['2_way_label'] == 1).sum():,}\n\n") # Linguistic features summary if 'linguistic_features' in self.analysis_results: f.write("## Linguistic Features Summary\n\n") # Basic statistics if 'basic_statistics' in self.analysis_results['linguistic_features']: stats = self.analysis_results['linguistic_features']['basic_statistics'] if 'by_category' in stats: f.write("### Text Length Analysis\n\n") for category, data in stats['by_category'].items(): f.write(f"**{category} Content:**\n") f.write(f"- Average character length: {data['avg_char_length']:.1f}\n") f.write(f"- Average word count: {data['avg_word_count']:.1f}\n") f.write(f"- Sample count: {data['sample_count']:,}\n\n") # Readability metrics if 'readability_metrics' in self.analysis_results['linguistic_features']: readability = self.analysis_results['linguistic_features']['readability_metrics'] if 'by_category' in readability and readability['by_category']: f.write("### Readability Analysis\n\n") for category, data in readability['by_category'].items(): f.write(f"**{category} Content:**\n") f.write(f"- Flesch Reading Ease: {data['flesch_reading_ease']['mean']:.1f}\n") f.write(f"- Flesch-Kincaid Grade: {data['flesch_kincaid_grade']['mean']:.1f}\n") f.write(f"- Sample size: {data['sample_size']}\n\n") # Authenticity patterns summary if 'authenticity_patterns' in self.analysis_results: f.write("## Authenticity Patterns\n\n") # Distinguishing features if 'distinguishing_features' in self.analysis_results['authenticity_patterns']: features = self.analysis_results['authenticity_patterns']['distinguishing_features'] if 'length_patterns' in features: length_diff = features['length_patterns']['length_difference'] rel_diff = features['length_patterns']['relative_difference'] f.write(f"### Key Findings\n\n") f.write(f"- False content is on average {length_diff:.1f} characters ") f.write(f"{'longer' if length_diff > 0 else 'shorter'} than true content\n") f.write(f"- This represents a {abs(rel_diff):.1f}% difference in length\n\n") # Top distinctive features if 'linguistic_signatures' in self.analysis_results['authenticity_patterns']: signatures = self.analysis_results['authenticity_patterns']['linguistic_signatures'] if 'tfidf_signatures' in signatures and 'true_distinctive_features' in signatures['tfidf_signatures']: f.write("### Distinctive Language Features\n\n") true_features = signatures['tfidf_signatures']['true_distinctive_features'][:5] false_features = signatures['tfidf_signatures']['false_distinctive_features'][:5] if true_features: f.write("**True Content Characteristics:**\n") for feature, score, _ in true_features: f.write(f"- {feature} (score: {score:.3f})\n") f.write("\n") if false_features: f.write("**False Content Characteristics:**\n") for feature, score, _ in false_features: f.write(f"- {feature} (score: {score:.3f})\n") f.write("\n") f.write("## Visualizations\n\n") f.write("The following visualizations have been generated:\n") f.write("- `text_basic_statistics.png` - Basic text statistics comparison\n") if TEXTSTAT_AVAILABLE: f.write("- `text_readability_metrics.png` - Readability analysis\n") if self.sentiment_analyzer: f.write("- `text_sentiment_analysis.png` - Sentiment comparison\n") f.write("- `text_keyword_analysis.png` - Top keywords by category\n") f.write("- `text_distinguishing_features.png` - Key distinguishing features\n\n") f.write("## Methodology\n\n") f.write("This analysis used the following approaches:\n") f.write("- **Linguistic Feature Extraction:** Character/word counts, readability metrics, sentiment analysis\n") f.write("- **Pattern Detection:** TF-IDF analysis, keyword frequency\n") f.write("- **Statistical Comparison:** Category differences analysis\n") f.write("- **Visualization:** Comparative charts, distribution plots\n") def main(): """Main function to run the complete text analysis pipeline.""" print("Starting Comprehensive Text Analysis Pipeline...") analyzer = TextAnalyzer() if not analyzer.load_processed_data(): print("Error: Failed to load data. Exiting.") return False # Extract linguistic features linguistic_features = analyzer.extract_linguistic_features() # Detect authenticity patterns authenticity_patterns = analyzer.detect_authenticity_patterns() # Create visualizations analyzer.create_text_visualizations() # Save results analyzer.save_analysis_results() print("Text analysis pipeline completed successfully!") print(f"Results saved to: {analyzer.output_dir}/") print(f"Visualizations saved to: {analyzer.viz_dir}/") return True if __name__ == "__main__": main()