#!/usr/bin/env python3 """ Fix Image-Text Mapping for Proper Multimodal Analysis This script creates proper mapping between text records and their corresponding images, ensuring we analyze the actual images associated with our dataset records. """ import pandas as pd import numpy as np from pathlib import Path import os import re from urllib.parse import urlparse import json def extract_image_id_from_url(url): """Extract image ID from various URL formats.""" if pd.isna(url): return None # Handle different URL patterns url = str(url) # Reddit preview URLs if 'preview.redd.it' in url or 'external-preview.redd.it' in url: # Extract the image ID from Reddit URLs match = re.search(r'/([a-zA-Z0-9_-]+)\.(jpg|jpeg|png|gif)', url) if match: return match.group(1) # Imgur URLs elif 'imgur.com' in url: match = re.search(r'/([a-zA-Z0-9]+)\.(jpg|jpeg|png|gif)', url) if match: return match.group(1) # Tumblr URLs elif 'tumblr.com' in url: match = re.search(r'/([a-zA-Z0-9_-]+)\.(jpg|jpeg|png|gif)', url) if match: return match.group(1) # Generic URL parsing else: parsed = urlparse(url) filename = Path(parsed.path).stem if filename: return filename return None def find_matching_image_files(record_ids, images_dir): """Find local image files that match our dataset record IDs.""" images_path = Path(images_dir) if not images_path.exists(): print(f"Images directory not found: {images_dir}") return {} print(f"Scanning {images_dir} for matching images...") print(f"Looking for {len(record_ids)} record IDs...") # Convert record IDs to set for faster lookup record_ids_set = set(record_ids) # Create mapping from record ID to image file path id_to_file = {} # Check for each record ID with different extensions for record_id in record_ids: for ext in ['.jpg', '.jpeg', '.png', '.gif']: img_file = images_path / f"{record_id}{ext}" if img_file.exists(): id_to_file[record_id] = img_file break # Found the image, no need to check other extensions print(f"Matched {len(id_to_file)} images to dataset records") print(f"Mapping success rate: {len(id_to_file)/len(record_ids)*100:.1f}%") return id_to_file def analyze_image_text_mapping(): """Analyze the mapping between text records and images.""" # Load the clean datasets analysis_dir = Path("analysis_results") datasets = {} for split in ['train', 'validation', 'test']: file_path = analysis_dir / f"{split}_clean.parquet" if file_path.exists(): datasets[split] = pd.read_parquet(file_path) if not datasets: print("No clean datasets found!") return # Combine all datasets for analysis all_data = pd.concat(datasets.values(), ignore_index=True) print(f"Total records: {len(all_data)}") print(f"Records with image URLs: {all_data['image_url'].notna().sum()}") print(f"Unique image URLs: {all_data['image_url'].nunique()}") # Use record IDs directly (they should match image filenames) print("\nUsing record IDs for image mapping...") record_ids = all_data['id'].dropna().unique() print(f"Record IDs to match: {len(record_ids)}") # Find matching local image files images_dir = os.getenv('IMAGES_FOLDER_PATH', '../Projects/public_image_set') id_to_file = find_matching_image_files(record_ids, images_dir) # Calculate mapping statistics total_records = len(all_data) records_with_urls = all_data['image_url'].notna().sum() records_with_ids = len(record_ids) matched_images = len(id_to_file) # Create mapping report mapping_report = { "analysis_timestamp": pd.Timestamp.now().isoformat(), "dataset_statistics": { "total_records": int(total_records), "records_with_image_urls": int(records_with_urls), "records_with_extractable_ids": int(records_with_ids), "unique_image_urls": int(all_data['image_url'].nunique()), "unique_image_ids": int(len(valid_image_ids)) }, "local_image_mapping": { "total_local_images": len(list(Path(images_dir).glob('*.*'))) if Path(images_dir).exists() else 0, "matched_images": matched_images, "mapping_rate": matched_images / records_with_ids * 100 if records_with_ids > 0 else 0 }, "mapping_challenges": [ "URL formats vary (Reddit, Imgur, Tumblr, etc.)", "Local filenames may not match URL patterns exactly", "Some images may have been removed or renamed", "Multiple posts may reference the same image" ], "recommendations": [ "Use available image URLs for analysis when local files not found", "Focus analysis on successfully mapped images", "Consider downloading missing images if needed", "Use image metadata and URL patterns for feature extraction" ] } # Add sample mappings for verification sample_mappings = [] for i, (record_id, file_path) in enumerate(list(id_to_file.items())[:10]): # Find corresponding record record = all_data[all_data['id'] == record_id].iloc[0] sample_mappings.append({ "record_id": record_id, "image_url": record['image_url'] if 'image_url' in record else None, "local_file": str(file_path), "clean_title": record['clean_title'][:100] + "..." if len(record['clean_title']) > 100 else record['clean_title'] }) mapping_report["sample_mappings"] = sample_mappings # Analyze comments mapping print("\nAnalyzing comments mapping...") comments_file = os.getenv('COMMENTS_TSV_PATH', '../Projects/all_comments.tsv') if Path(comments_file).exists(): try: # Load a sample of comments to check structure comments_sample = pd.read_csv(comments_file, sep='\t', nrows=1000) print(f"Comments file columns: {list(comments_sample.columns)}") if 'submission_id' in comments_sample.columns: # Check how many of our record IDs have comments our_ids_with_comments = comments_sample['submission_id'].isin(record_ids).sum() unique_submissions_in_sample = comments_sample['submission_id'].nunique() mapping_report["comments_mapping"] = { "comments_file_exists": True, "sample_comments_analyzed": len(comments_sample), "our_records_with_comments_in_sample": int(our_ids_with_comments), "unique_submissions_in_sample": int(unique_submissions_in_sample), "comments_columns": list(comments_sample.columns) } print(f"Comments in sample: {len(comments_sample)}") print(f"Our records with comments in sample: {our_ids_with_comments}") else: mapping_report["comments_mapping"] = { "comments_file_exists": True, "error": "submission_id column not found", "available_columns": list(comments_sample.columns) } except Exception as e: mapping_report["comments_mapping"] = { "comments_file_exists": True, "error": str(e) } else: mapping_report["comments_mapping"] = { "comments_file_exists": False, "comments_file_path": comments_file } # Save the mapping report with open(analysis_dir / "image_text_mapping_analysis.json", 'w', encoding='utf-8') as f: json.dump(mapping_report, f, indent=2, default=str) # Print summary print(f"\n" + "="*60) print("IMAGE-TEXT MAPPING ANALYSIS SUMMARY") print("="*60) print(f"Total Records: {total_records:,}") print(f"Records with Image URLs: {records_with_urls:,} ({records_with_urls/total_records*100:.1f}%)") print(f"Extractable Image IDs: {records_with_ids:,} ({records_with_ids/total_records*100:.1f}%)") print(f"Local Images Found: {matched_images:,}") print(f"Mapping Success Rate: {matched_images/records_with_ids*100:.1f}%" if records_with_ids > 0 else "N/A") print(f"\nKey Insight:") if matched_images < records_with_ids * 0.5: print(f" Warning: Low mapping rate suggests image analysis should focus on") print(f" available images or use URL-based features") else: print(f" Good mapping rate allows for proper image-text analysis") print(f"\nRecommendation:") print(f" Use the {matched_images:,} successfully mapped images for") print(f" proper multimodal analysis instead of random sampling") return mapping_report if __name__ == "__main__": analyze_image_text_mapping()