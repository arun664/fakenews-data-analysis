""" Data Processing Utilities This module provides utility functions for efficient handling of large datasets, memory management, and data processing operations. Requirements addressed: 1.1, 1.2, 1.4, 1.5 """ import pandas as pd import numpy as np import os import psutil import logging from typing import Dict, List, Optional, Tuple, Iterator, Any from pathlib import Path import gc from contextlib import contextmanager logger = logging.getLogger(__name__) class MemoryManager: """Manages memory usage during data processing operations""" def __init__(self, memory_limit_gb: float = 8.0): self.memory_limit_gb = memory_limit_gb self.memory_limit_bytes = memory_limit_gb * 1024 * 1024 * 1024 def get_memory_usage(self) -> Dict[str, float]: """Get current memory usage statistics""" process = psutil.Process() memory_info = process.memory_info() return { 'rss_gb': memory_info.rss / 1024 / 1024 / 1024, 'vms_gb': memory_info.vms / 1024 / 1024 / 1024, 'percent': process.memory_percent(), 'available_gb': psutil.virtual_memory().available / 1024 / 1024 / 1024 } def check_memory_limit(self) -> bool: """Check if memory usage is within limits""" usage = self.get_memory_usage() return usage['rss_gb'] < self.memory_limit_gb @contextmanager def memory_monitor(self, operation_name: str): """Context manager for monitoring memory usage during operations""" initial_usage = self.get_memory_usage() logger.info(f"Starting {operation_name} - Memory usage: {initial_usage['rss_gb']:.2f} GB") try: yield finally: final_usage = self.get_memory_usage() memory_delta = final_usage['rss_gb'] - initial_usage['rss_gb'] logger.info(f"Completed {operation_name} - Memory usage: {final_usage['rss_gb']:.2f} GB (Î”{memory_delta:+.2f} GB)") # Force garbage collection if memory usage is high if final_usage['rss_gb'] > self.memory_limit_gb * 0.8: gc.collect() logger.info("Performed garbage collection due to high memory usage") def estimate_dataframe_memory(df: pd.DataFrame) -> Dict[str, float]: """ Estimate memory usage of a DataFrame. Args: df: DataFrame to analyze Returns: Dictionary with memory usage statistics """ memory_usage = df.memory_usage(deep=True) return { 'total_mb': memory_usage.sum() / 1024 / 1024, 'index_mb': memory_usage.iloc[0] / 1024 / 1024, 'columns_mb': memory_usage.iloc[1:].sum() / 1024 / 1024, 'per_column_mb': (memory_usage.iloc[1:] / 1024 / 1024).to_dict() } def optimize_dataframe_dtypes(df: pd.DataFrame) -> pd.DataFrame: """ Optimize DataFrame data types to reduce memory usage. Args: df: DataFrame to optimize Returns: DataFrame with optimized data types """ logger.info("Optimizing DataFrame data types") optimized_df = df.copy() # Optimize integer columns int_cols = optimized_df.select_dtypes(include=['int64']).columns for col in int_cols: col_min = optimized_df[col].min() col_max = optimized_df[col].max() if col_min >= 0: # Unsigned integers if col_max < 255: optimized_df[col] = optimized_df[col].astype('uint8') elif col_max < 65535: optimized_df[col] = optimized_df[col].astype('uint16') elif col_max < 4294967295: optimized_df[col] = optimized_df[col].astype('uint32') else: # Signed integers if col_min > -128 and col_max < 127: optimized_df[col] = optimized_df[col].astype('int8') elif col_min > -32768 and col_max < 32767: optimized_df[col] = optimized_df[col].astype('int16') elif col_min > -2147483648 and col_max < 2147483647: optimized_df[col] = optimized_df[col].astype('int32') # Optimize float columns float_cols = optimized_df.select_dtypes(include=['float64']).columns for col in float_cols: optimized_df[col] = pd.to_numeric(optimized_df[col], downcast='float') # Convert object columns to category where appropriate object_cols = optimized_df.select_dtypes(include=['object']).columns for col in object_cols: unique_ratio = optimized_df[col].nunique() / len(optimized_df) if unique_ratio < 0.5: # If less than 50% unique values, convert to category optimized_df[col] = optimized_df[col].astype('category') # Log memory savings original_memory = estimate_dataframe_memory(df) optimized_memory = estimate_dataframe_memory(optimized_df) memory_savings = original_memory['total_mb'] - optimized_memory['total_mb'] logger.info(f"Memory optimization completed. Saved {memory_savings:.2f} MB ({memory_savings/original_memory['total_mb']*100:.1f}%)") return optimized_df def chunked_dataframe_processor(file_path: Path, chunk_size: int = 10000, processing_func: Optional[callable] = None) -> Iterator[pd.DataFrame]: """ Process large files in chunks to manage memory usage. Args: file_path: Path to the file to process chunk_size: Size of each chunk processing_func: Optional function to apply to each chunk Yields: Processed DataFrame chunks """ logger.info(f"Processing file in chunks: {file_path}") chunk_count = 0 try: for chunk in pd.read_csv(file_path, sep='\t', chunksize=chunk_size, low_memory=False): chunk_count += 1 logger.debug(f"Processing chunk {chunk_count}, size: {len(chunk)}") # Apply processing function if provided if processing_func: chunk = processing_func(chunk) # Optimize memory usage chunk = optimize_dataframe_dtypes(chunk) yield chunk # Periodic garbage collection if chunk_count % 10 == 0: gc.collect() except Exception as e: logger.error(f"Error processing file {file_path}: {str(e)}") raise def validate_file_integrity(file_path: Path) -> Dict[str, Any]: """ Validate file integrity and basic properties. Args: file_path: Path to file to validate Returns: Dictionary with validation results """ logger.info(f"Validating file integrity: {file_path}") validation_result = { 'exists': file_path.exists(), 'readable': False, 'size_mb': 0, 'estimated_rows': 0, 'columns': [], 'errors': [] } if not validation_result['exists']: validation_result['errors'].append(f"File does not exist: {file_path}") return validation_result try: # Check file size file_size = file_path.stat().st_size validation_result['size_mb'] = file_size / 1024 / 1024 # Read first few rows to validate structure sample_df = pd.read_csv(file_path, sep='\t', nrows=100, low_memory=False) validation_result['readable'] = True validation_result['columns'] = sample_df.columns.tolist() # Estimate total rows (rough estimate based on sample) avg_row_size = file_size / len(sample_df) if len(sample_df) > 0 else 0 validation_result['estimated_rows'] = int(file_size / avg_row_size) if avg_row_size > 0 else 0 logger.info(f"File validation successful: {validation_result['size_mb']:.2f} MB, ~{validation_result['estimated_rows']} rows") except Exception as e: validation_result['errors'].append(f"Error reading file: {str(e)}") logger.error(f"File validation failed: {str(e)}") return validation_result def create_data_summary(df: pd.DataFrame) -> Dict[str, Any]: """ Create comprehensive data summary statistics. Args: df: DataFrame to summarize Returns: Dictionary with summary statistics """ logger.info("Creating data summary") summary = { 'shape': df.shape, 'memory_usage': estimate_dataframe_memory(df), 'column_info': {}, 'missing_data': {}, 'data_types': df.dtypes.astype(str).to_dict(), 'numeric_summary': {}, 'categorical_summary': {} } # Column-level information for col in df.columns: col_info = { 'dtype': str(df[col].dtype), 'non_null_count': df[col].count(), 'null_count': df[col].isnull().sum(), 'null_percentage': df[col].isnull().sum() / len(df) * 100, 'unique_count': df[col].nunique(), 'unique_percentage': df[col].nunique() / len(df) * 100 } summary['column_info'][col] = col_info summary['missing_data'][col] = col_info['null_percentage'] # Numeric column summaries numeric_cols = df.select_dtypes(include=[np.number]).columns for col in numeric_cols: if df[col].count() > 0: summary['numeric_summary'][col] = { 'mean': df[col].mean(), 'median': df[col].median(), 'std': df[col].std(), 'min': df[col].min(), 'max': df[col].max(), 'q25': df[col].quantile(0.25), 'q75': df[col].quantile(0.75) } # Categorical column summaries categorical_cols = df.select_dtypes(include=['object', 'category']).columns for col in categorical_cols: if df[col].count() > 0: value_counts = df[col].value_counts().head(10) summary['categorical_summary'][col] = { 'top_values': value_counts.to_dict(), 'most_frequent': value_counts.index[0] if len(value_counts) > 0 else None, 'most_frequent_count': value_counts.iloc[0] if len(value_counts) > 0 else 0 } logger.info("Data summary creation completed") return summary def save_processed_data(df: pd.DataFrame, output_path: Path, format: str = 'parquet', compression: str = 'snappy') -> bool: """ Save processed data in efficient format. Args: df: DataFrame to save output_path: Output file path format: Output format ('parquet', 'csv', 'pickle') compression: Compression method Returns: Success status """ logger.info(f"Saving processed data to {output_path}") try: output_path.parent.mkdir(parents=True, exist_ok=True) if format.lower() == 'parquet': try: df.to_parquet(output_path, compression=compression, index=False) except ImportError: # Fall back to CSV if parquet libraries not available logger.warning("Parquet libraries not available, falling back to CSV format") csv_path = output_path.with_suffix('.csv') df.to_csv(csv_path, index=False) logger.info(f"Saved as CSV: {csv_path}") elif format.lower() == 'csv': # Use appropriate compression for CSV csv_compression = 'gzip' if compression == 'snappy' else compression df.to_csv(output_path, index=False, compression=csv_compression) elif format.lower() == 'pickle': df.to_pickle(output_path, compression=compression) else: raise ValueError(f"Unsupported format: {format}") logger.info(f"Successfully saved {len(df)} records to {output_path}") return True except Exception as e: logger.error(f"Error saving data: {str(e)}") return False def load_processed_data(file_path: Path, format: str = 'parquet') -> Optional[pd.DataFrame]: """ Load processed data from file. Args: file_path: Path to data file format: File format ('parquet', 'csv', 'pickle') Returns: Loaded DataFrame or None if error """ logger.info(f"Loading processed data from {file_path}") try: if format.lower() == 'parquet': df = pd.read_parquet(file_path) elif format.lower() == 'csv': df = pd.read_csv(file_path) elif format.lower() == 'pickle': df = pd.read_pickle(file_path) else: raise ValueError(f"Unsupported format: {format}") logger.info(f"Successfully loaded {len(df)} records from {file_path}") return df except Exception as e: logger.error(f"Error loading data: {str(e)}") return None