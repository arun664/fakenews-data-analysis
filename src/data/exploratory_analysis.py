""" Exploratory Data Analysis Module This module provides comprehensive exploratory data analysis capabilities for the Fakeddit dataset, focusing on descriptive statistics, visualizations, and pattern discovery across misinformation categories. """ import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from typing import Dict, List, Tuple, Optional import warnings from pathlib import Path import json from datetime import datetime warnings.filterwarnings('ignore') class ExploratoryAnalyzer: """ Comprehensive exploratory data analysis for multimodal fake news detection dataset. This class provides methods to analyze data distributions, generate descriptive statistics, create visualizations, and identify patterns across different misinformation categories. """ def __init__(self, data_dir: str = "processed_data"): """ Initialize the exploratory analyzer. Args: data_dir: Directory containing the clean dataset files """ self.data_dir = Path(data_dir) self.train_data = None self.validation_data = None self.test_data = None self.combined_data = None self.category_mapping = { 0: "True", 1: "Satire", 2: "Misleading", 3: "False" } def load_clean_datasets(self) -> bool: """ Load the clean datasets from parquet files. Returns: bool: True if datasets loaded successfully, False otherwise """ try: self.train_data = pd.read_parquet(self.data_dir / "train_clean.parquet") self.validation_data = pd.read_parquet(self.data_dir / "validation_clean.parquet") self.test_data = pd.read_parquet(self.data_dir / "test_clean.parquet") # Add split identifier self.train_data['split'] = 'train' self.validation_data['split'] = 'validation' self.test_data['split'] = 'test' # Combine all data for comprehensive analysis self.combined_data = pd.concat([ self.train_data, self.validation_data, self.test_data ], ignore_index=True) print(f"Loaded datasets successfully:") print(f" Train: {len(self.train_data)} records") print(f" Validation: {len(self.validation_data)} records") print(f" Test: {len(self.test_data)} records") print(f" Combined: {len(self.combined_data)} records") return True except Exception as e: print(f"Error loading datasets: {e}") return False def generate_descriptive_statistics(self) -> Dict: """ Generate comprehensive descriptive statistics for the dataset. Returns: Dict: Dictionary containing descriptive statistics """ if self.combined_data is None: raise ValueError("Data not loaded. Call load_clean_datasets() first.") stats = {} # Overall dataset statistics stats['dataset_overview'] = { 'total_records': len(self.combined_data), 'total_features': len(self.combined_data.columns), 'memory_usage_mb': self.combined_data.memory_usage(deep=True).sum() / 1024**2, 'split_distribution': self.combined_data['split'].value_counts().to_dict() } # Category distribution analysis if '2_way_label' in self.combined_data.columns: stats['category_distribution'] = { 'overall': self.combined_data['2_way_label'].value_counts().to_dict(), 'by_split': {} } for split in ['train', 'validation', 'test']: split_data = self.combined_data[self.combined_data['split'] == split] if len(split_data) > 0: stats['category_distribution']['by_split'][split] = \ split_data['2_way_label'].value_counts().to_dict() # Multi-way label analysis if available if '6_way_label' in self.combined_data.columns: stats['multiway_distribution'] = { 'overall': self.combined_data['6_way_label'].value_counts().to_dict(), 'by_split': {} } for split in ['train', 'validation', 'test']: split_data = self.combined_data[self.combined_data['split'] == split] if len(split_data) > 0: stats['multiway_distribution']['by_split'][split] = \ split_data['6_way_label'].value_counts().to_dict() # Text content analysis text_columns = ['clean_title', 'text', 'title'] available_text_cols = [col for col in text_columns if col in self.combined_data.columns] if available_text_cols: stats['text_analysis'] = {} for col in available_text_cols: if self.combined_data[col].notna().any(): text_data = self.combined_data[col].dropna().astype(str) stats['text_analysis'][col] = { 'total_entries': len(text_data), 'avg_length': text_data.str.len().mean(), 'median_length': text_data.str.len().median(), 'max_length': text_data.str.len().max(), 'min_length': text_data.str.len().min(), 'std_length': text_data.str.len().std() } # Numerical features analysis numerical_cols = self.combined_data.select_dtypes(include=[np.number]).columns if len(numerical_cols) > 0: stats['numerical_features'] = {} for col in numerical_cols: if col not in ['2_way_label', '6_way_label']: # Exclude target variables stats['numerical_features'][col] = { 'mean': float(self.combined_data[col].mean()), 'median': float(self.combined_data[col].median()), 'std': float(self.combined_data[col].std()), 'min': float(self.combined_data[col].min()), 'max': float(self.combined_data[col].max()), 'missing_count': int(self.combined_data[col].isna().sum()), 'missing_percentage': float(self.combined_data[col].isna().mean() * 100) } # Data quality metrics stats['data_quality'] = { 'missing_values_by_column': self.combined_data.isnull().sum().to_dict(), 'missing_percentage_by_column': (self.combined_data.isnull().mean() * 100).to_dict(), 'duplicate_rows': int(self.combined_data.duplicated().sum()), 'unique_values_by_column': {} } # Unique values for categorical columns categorical_cols = self.combined_data.select_dtypes(include=['object', 'category']).columns for col in categorical_cols: unique_count = self.combined_data[col].nunique() stats['data_quality']['unique_values_by_column'][col] = unique_count return stats def analyze_category_patterns(self) -> Dict: """ Analyze patterns specific to each misinformation category. Returns: Dict: Category-specific pattern analysis """ if self.combined_data is None: raise ValueError("Data not loaded. Call load_clean_datasets() first.") patterns = {} # Analyze by 2-way labels (True/False) if '2_way_label' in self.combined_data.columns: patterns['binary_patterns'] = {} for label in self.combined_data['2_way_label'].unique(): if pd.notna(label): subset = self.combined_data[self.combined_data['2_way_label'] == label] label_name = "True" if label == 0 else "False" patterns['binary_patterns'][label_name] = { 'count': len(subset), 'percentage': len(subset) / len(self.combined_data) * 100 } # Text length analysis by category text_cols = ['clean_title', 'text', 'title'] available_text_cols = [col for col in text_cols if col in subset.columns] if available_text_cols: patterns['binary_patterns'][label_name]['text_characteristics'] = {} for col in available_text_cols: if subset[col].notna().any(): text_data = subset[col].dropna().astype(str) patterns['binary_patterns'][label_name]['text_characteristics'][col] = { 'avg_length': float(text_data.str.len().mean()), 'median_length': float(text_data.str.len().median()), 'std_length': float(text_data.str.len().std()) } # Analyze by 6-way labels if available if '6_way_label' in self.combined_data.columns: patterns['multiway_patterns'] = {} for label in self.combined_data['6_way_label'].unique(): if pd.notna(label): subset = self.combined_data[self.combined_data['6_way_label'] == label] label_name = self.category_mapping.get(label, f"Category_{label}") patterns['multiway_patterns'][label_name] = { 'count': len(subset), 'percentage': len(subset) / len(self.combined_data) * 100 } # Additional analysis for each category if len(subset) > 0: # Numerical features by category numerical_cols = subset.select_dtypes(include=[np.number]).columns numerical_cols = [col for col in numerical_cols if col not in ['2_way_label', '6_way_label']] if len(numerical_cols) > 0: patterns['multiway_patterns'][label_name]['numerical_summary'] = {} for col in numerical_cols: if subset[col].notna().any(): patterns['multiway_patterns'][label_name]['numerical_summary'][col] = { 'mean': float(subset[col].mean()), 'std': float(subset[col].std()), 'median': float(subset[col].median()) } return patterns def create_visualizations(self, output_dir: str = "eda_output") -> bool: """ Create comprehensive visualizations for the dataset. Args: output_dir: Directory to save visualization files Returns: bool: True if visualizations created successfully """ if self.combined_data is None: raise ValueError("Data not loaded. Call load_clean_datasets() first.") output_path = Path(output_dir) output_path.mkdir(exist_ok=True) # Set style for better-looking plots plt.style.use('default') sns.set_palette("husl") try: # 1. Dataset split distribution plt.figure(figsize=(10, 6)) split_counts = self.combined_data['split'].value_counts() plt.subplot(1, 2, 1) split_counts.plot(kind='bar', color=['skyblue', 'lightcoral', 'lightgreen']) plt.title('Dataset Split Distribution') plt.xlabel('Split') plt.ylabel('Number of Records') plt.xticks(rotation=45) # 2. Category distribution (2-way) if '2_way_label' in self.combined_data.columns: plt.subplot(1, 2, 2) category_counts = self.combined_data['2_way_label'].value_counts() labels = ['True' if x == 0 else 'False' for x in category_counts.index] plt.pie(category_counts.values, labels=labels, autopct='%1.1f%%', startangle=90) plt.title('Binary Label Distribution') plt.tight_layout() plt.savefig(output_path / 'dataset_overview.png', dpi=300, bbox_inches='tight') plt.close() # 3. Multi-way category distribution if '6_way_label' in self.combined_data.columns: plt.figure(figsize=(12, 8)) # Overall distribution plt.subplot(2, 2, 1) multiway_counts = self.combined_data['6_way_label'].value_counts().sort_index() category_names = [self.category_mapping.get(x, f"Cat_{x}") for x in multiway_counts.index] plt.bar(category_names, multiway_counts.values, color='lightblue') plt.title('6-Way Category Distribution') plt.xlabel('Category') plt.ylabel('Count') plt.xticks(rotation=45) # Distribution by split for i, split in enumerate(['train', 'validation', 'test'], 2): plt.subplot(2, 2, i) split_data = self.combined_data[self.combined_data['split'] == split] if len(split_data) > 0: split_counts = split_data['6_way_label'].value_counts().sort_index() split_names = [self.category_mapping.get(x, f"Cat_{x}") for x in split_counts.index] plt.bar(split_names, split_counts.values, color='lightcoral') plt.title(f'{split.capitalize()} Split Distribution') plt.xlabel('Category') plt.ylabel('Count') plt.xticks(rotation=45) plt.tight_layout() plt.savefig(output_path / 'category_distributions.png', dpi=300, bbox_inches='tight') plt.close() # 4. Text length analysis text_columns = ['clean_title', 'text', 'title'] available_text_cols = [col for col in text_columns if col in self.combined_data.columns] if available_text_cols: fig, axes = plt.subplots(len(available_text_cols), 2, figsize=(15, 5*len(available_text_cols))) if len(available_text_cols) == 1: axes = axes.reshape(1, -1) for i, col in enumerate(available_text_cols): if self.combined_data[col].notna().any(): text_lengths = self.combined_data[col].dropna().astype(str).str.len() # Histogram axes[i, 0].hist(text_lengths, bins=50, alpha=0.7, color='skyblue') axes[i, 0].set_title(f'{col} Length Distribution') axes[i, 0].set_xlabel('Character Length') axes[i, 0].set_ylabel('Frequency') # Box plot by category if available if '2_way_label' in self.combined_data.columns: data_with_lengths = self.combined_data.copy() data_with_lengths[f'{col}_length'] = data_with_lengths[col].astype(str).str.len() data_with_lengths['category'] = data_with_lengths['2_way_label'].map({0: 'True', 1: 'False'}) sns.boxplot(data=data_with_lengths, x='category', y=f'{col}_length', ax=axes[i, 1]) axes[i, 1].set_title(f'{col} Length by Category') axes[i, 1].set_xlabel('Category') axes[i, 1].set_ylabel('Character Length') plt.tight_layout() plt.savefig(output_path / 'text_analysis.png', dpi=300, bbox_inches='tight') plt.close() # 5. Missing values heatmap plt.figure(figsize=(12, 8)) missing_data = self.combined_data.isnull() sns.heatmap(missing_data.corr(), annot=True, cmap='coolwarm', center=0) plt.title('Missing Values Correlation Heatmap') plt.tight_layout() plt.savefig(output_path / 'missing_values_heatmap.png', dpi=300, bbox_inches='tight') plt.close() # 6. Data quality overview plt.figure(figsize=(14, 10)) # Missing values by column plt.subplot(2, 2, 1) missing_counts = self.combined_data.isnull().sum() missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=True) if len(missing_counts) > 0: missing_counts.plot(kind='barh', color='lightcoral') plt.title('Missing Values by Column') plt.xlabel('Number of Missing Values') else: plt.text(0.5, 0.5, 'No Missing Values', ha='center', va='center', transform=plt.gca().transAxes) plt.title('Missing Values by Column') # Data types distribution plt.subplot(2, 2, 2) dtype_counts = self.combined_data.dtypes.value_counts() plt.pie(dtype_counts.values, labels=dtype_counts.index, autopct='%1.1f%%') plt.title('Data Types Distribution') # Unique values per column (for categorical) plt.subplot(2, 2, 3) categorical_cols = self.combined_data.select_dtypes(include=['object', 'category']).columns if len(categorical_cols) > 0: unique_counts = self.combined_data[categorical_cols].nunique().sort_values(ascending=True) unique_counts.plot(kind='barh', color='lightgreen') plt.title('Unique Values in Categorical Columns') plt.xlabel('Number of Unique Values') else: plt.text(0.5, 0.5, 'No Categorical Columns', ha='center', va='center', transform=plt.gca().transAxes) plt.title('Unique Values in Categorical Columns') # Memory usage by column plt.subplot(2, 2, 4) memory_usage = self.combined_data.memory_usage(deep=True).sort_values(ascending=True) memory_usage_mb = memory_usage / 1024**2 memory_usage_mb.plot(kind='barh', color='gold') plt.title('Memory Usage by Column (MB)') plt.xlabel('Memory Usage (MB)') plt.tight_layout() plt.savefig(output_path / 'data_quality_overview.png', dpi=300, bbox_inches='tight') plt.close() print(f"Visualizations saved to {output_path}/") return True except Exception as e: print(f"Error creating visualizations: {e}") return False def generate_comprehensive_report(self, output_dir: str = "eda_output") -> bool: """ Generate a comprehensive EDA report combining statistics and analysis. Args: output_dir: Directory to save the report Returns: bool: True if report generated successfully """ if self.combined_data is None: raise ValueError("Data not loaded. Call load_clean_datasets() first.") output_path = Path(output_dir) output_path.mkdir(exist_ok=True) try: # Generate all analyses descriptive_stats = self.generate_descriptive_statistics() category_patterns = self.analyze_category_patterns() # Create comprehensive report report = { 'analysis_timestamp': datetime.now().isoformat(), 'dataset_summary': descriptive_stats, 'category_patterns': category_patterns, 'data_quality_assessment': self._assess_data_quality(), 'preprocessing_recommendations': self._generate_preprocessing_recommendations() } # Save JSON report with open(output_path / 'eda_comprehensive_report.json', 'w') as f: json.dump(report, f, indent=2, default=str) # Generate markdown report self._generate_markdown_report(report, output_path / 'EDA_REPORT.md') print(f"Comprehensive EDA report saved to {output_path}/") return True except Exception as e: print(f"Error generating comprehensive report: {e}") return False def _assess_data_quality(self) -> Dict: """Assess overall data quality and identify issues.""" quality_assessment = { 'overall_score': 0.0, 'issues_identified': [], 'strengths': [], 'recommendations': [] } # Calculate quality score based on various factors score_components = [] # Missing values assessment missing_percentage = self.combined_data.isnull().mean().mean() * 100 if missing_percentage < 5: score_components.append(0.3) quality_assessment['strengths'].append(f"Low missing values ({missing_percentage:.1f}%)") elif missing_percentage < 15: score_components.append(0.2) quality_assessment['issues_identified'].append(f"Moderate missing values ({missing_percentage:.1f}%)") else: score_components.append(0.1) quality_assessment['issues_identified'].append(f"High missing values ({missing_percentage:.1f}%)") # Duplicate assessment duplicate_percentage = self.combined_data.duplicated().mean() * 100 if duplicate_percentage < 1: score_components.append(0.2) quality_assessment['strengths'].append(f"Low duplicate records ({duplicate_percentage:.1f}%)") else: score_components.append(0.1) quality_assessment['issues_identified'].append(f"Duplicate records present ({duplicate_percentage:.1f}%)") # Category balance assessment if '2_way_label' in self.combined_data.columns: category_balance = self.combined_data['2_way_label'].value_counts(normalize=True) min_category_pct = category_balance.min() * 100 if min_category_pct > 30: score_components.append(0.3) quality_assessment['strengths'].append(f"Well-balanced categories (min: {min_category_pct:.1f}%)") elif min_category_pct > 15: score_components.append(0.2) quality_assessment['issues_identified'].append(f"Moderate class imbalance (min: {min_category_pct:.1f}%)") else: score_components.append(0.1) quality_assessment['issues_identified'].append(f"Severe class imbalance (min: {min_category_pct:.1f}%)") # Data consistency assessment score_components.append(0.2) # Assume reasonable consistency after cleaning quality_assessment['strengths'].append("Data passed leakage mitigation process") quality_assessment['overall_score'] = sum(score_components) return quality_assessment def _generate_preprocessing_recommendations(self) -> List[str]: """Generate preprocessing recommendations based on data analysis.""" recommendations = [] # Missing values recommendations missing_cols = self.combined_data.isnull().sum() high_missing_cols = missing_cols[missing_cols > len(self.combined_data) * 0.1] if len(high_missing_cols) > 0: recommendations.append(f"Consider imputation or removal for columns with >10% missing values: {list(high_missing_cols.index)}") # Text preprocessing recommendations text_columns = ['clean_title', 'text', 'title'] available_text_cols = [col for col in text_columns if col in self.combined_data.columns] if available_text_cols: recommendations.append("Apply text preprocessing: lowercasing, punctuation removal, tokenization") recommendations.append("Consider text normalization and stop word removal for analysis") # Category imbalance recommendations if '2_way_label' in self.combined_data.columns: category_balance = self.combined_data['2_way_label'].value_counts(normalize=True) if category_balance.min() < 0.3: recommendations.append("Consider sampling techniques to address class imbalance") # Feature engineering recommendations recommendations.append("Extract additional text features: length, word count, readability scores") recommendations.append("Consider temporal features if timestamp information is available") return recommendations def _generate_markdown_report(self, report_data: Dict, output_path: Path): """Generate a markdown report from the analysis data.""" markdown_content = f"""# Exploratory Data Analysis Report **Analysis Date:** {report_data['analysis_timestamp']} ## Dataset Overview ### Basic Statistics - **Total Records:** {report_data['dataset_summary']['dataset_overview']['total_records']:,} - **Total Features:** {report_data['dataset_summary']['dataset_overview']['total_features']} - **Memory Usage:** {report_data['dataset_summary']['dataset_overview']['memory_usage_mb']:.2f} MB ### Split Distribution """ for split, count in report_data['dataset_summary']['dataset_overview']['split_distribution'].items(): markdown_content += f"- **{split.capitalize()}:** {count:,} records\n" # Category distribution if 'category_distribution' in report_data['dataset_summary']: markdown_content += "\n### Category Distribution (Binary)\n" for category, count in report_data['dataset_summary']['category_distribution']['overall'].items(): category_name = "True" if category == 0 else "False" percentage = (count / report_data['dataset_summary']['dataset_overview']['total_records']) * 100 markdown_content += f"- **{category_name}:** {count:,} records ({percentage:.1f}%)\n" # Data quality assessment if 'data_quality_assessment' in report_data: quality = report_data['data_quality_assessment'] markdown_content += f"\n## Data Quality Assessment\n\n" markdown_content += f"**Overall Quality Score:** {quality['overall_score']:.2f}/1.0\n\n" if quality['strengths']: markdown_content += "### Strengths\n" for strength in quality['strengths']: markdown_content += f"- {strength}\n" if quality['issues_identified']: markdown_content += "\n### Issues Identified\n" for issue in quality['issues_identified']: markdown_content += f"- {issue}\n" # Preprocessing recommendations if 'preprocessing_recommendations' in report_data: markdown_content += "\n## Preprocessing Recommendations\n\n" for rec in report_data['preprocessing_recommendations']: markdown_content += f"- {rec}\n" # Text analysis summary if 'text_analysis' in report_data['dataset_summary']: markdown_content += "\n## Text Content Analysis\n\n" for col, stats in report_data['dataset_summary']['text_analysis'].items(): markdown_content += f"### {col}\n" markdown_content += f"- **Average Length:** {stats['avg_length']:.1f} characters\n" markdown_content += f"- **Median Length:** {stats['median_length']:.1f} characters\n" markdown_content += f"- **Max Length:** {stats['max_length']:,} characters\n" markdown_content += f"- **Min Length:** {stats['min_length']} characters\n\n" # Category patterns if 'binary_patterns' in report_data['category_patterns']: markdown_content += "\n## Category-Specific Patterns\n\n" for category, patterns in report_data['category_patterns']['binary_patterns'].items(): markdown_content += f"### {category} Content\n" markdown_content += f"- **Count:** {patterns['count']:,} records ({patterns['percentage']:.1f}%)\n" if 'text_characteristics' in patterns: markdown_content += "- **Text Characteristics:**\n" for text_col, chars in patterns['text_characteristics'].items(): markdown_content += f" - {text_col}: Avg {chars['avg_length']:.1f} chars, Median {chars['median_length']:.1f} chars\n" markdown_content += "\n" markdown_content += "\n## Visualizations\n\n" markdown_content += "The following visualizations have been generated:\n" markdown_content += "- `dataset_overview.png` - Dataset split and category distributions\n" markdown_content += "- `category_distributions.png` - Detailed category breakdowns\n" markdown_content += "- `text_analysis.png` - Text length distributions and patterns\n" markdown_content += "- `missing_values_heatmap.png` - Missing values correlation analysis\n" markdown_content += "- `data_quality_overview.png` - Comprehensive data quality metrics\n" # Write markdown file with open(output_path, 'w', encoding='utf-8') as f: f.write(markdown_content) def main(): """Main function to run exploratory data analysis.""" print("Starting Exploratory Data Analysis...") # Initialize analyzer analyzer = ExploratoryAnalyzer() # Load datasets if not analyzer.load_clean_datasets(): print("Failed to load datasets. Exiting.") return # Create output directory output_dir = "eda_output" Path(output_dir).mkdir(exist_ok=True) # Generate descriptive statistics print("\nGenerating descriptive statistics...") stats = analyzer.generate_descriptive_statistics() # Analyze category patterns print("Analyzing category patterns...") patterns = analyzer.analyze_category_patterns() # Create visualizations print("Creating visualizations...") analyzer.create_visualizations(output_dir) # Generate comprehensive report print("Generating comprehensive report...") analyzer.generate_comprehensive_report(output_dir) print(f"\nExploratory Data Analysis completed successfully!") print(f"Results saved to '{output_dir}/' directory") print("\nKey findings:") print(f"- Total records analyzed: {stats['dataset_overview']['total_records']:,}") print(f"- Dataset memory usage: {stats['dataset_overview']['memory_usage_mb']:.2f} MB") if 'category_distribution' in stats: print("- Category distribution:") for category, count in stats['category_distribution']['overall'].items(): category_name = "True" if category == 0 else "False" percentage = (count / stats['dataset_overview']['total_records']) * 100 print(f" - {category_name}: {count:,} records ({percentage:.1f}%)") if __name__ == "__main__": main()