""" Dataset Loading and Preprocessing Module for Fakeddit Dataset This module handles loading and preprocessing of the Fakeddit multimodal dataset including TSV files, image metadata, and comments data with proper handling of the 112GB dataset size. Requirements addressed: 1.1, 1.2, 1.4, 1.5 """ import pandas as pd import numpy as np import os import logging from typing import Dict, List, Optional, Tuple, Union from pathlib import Path import gc from dataclasses import dataclass from datetime import datetime from dotenv import load_dotenv # Load environment variables load_dotenv() # Configure logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) @dataclass class DatasetConfig: """Configuration for dataset loading and preprocessing""" base_dir: str = None train_tsv_path: str = None validation_tsv_path: str = None test_tsv_path: str = None comments_tsv_path: str = None images_folder_path: str = None chunk_size: int = 10000 memory_limit_gb: float = 8.0 categories: List[str] = None def __post_init__(self): # Load from environment variables if not provided if self.base_dir is None: self.base_dir = os.getenv('FAKEDDIT_BASE_DIR', './data/fakeddit') if self.train_tsv_path is None: self.train_tsv_path = os.getenv('TRAIN_TSV_PATH', f'{self.base_dir}/multimodal_only_samples_train.tsv') if self.validation_tsv_path is None: self.validation_tsv_path = os.getenv('VALIDATION_TSV_PATH', f'{self.base_dir}/multimodal_only_samples_validation.tsv') if self.test_tsv_path is None: self.test_tsv_path = os.getenv('TEST_TSV_PATH', f'{self.base_dir}/multimodal_only_samples_test.tsv') if self.comments_tsv_path is None: self.comments_tsv_path = os.getenv('COMMENTS_TSV_PATH', f'{self.base_dir}/comments.tsv') if self.images_folder_path is None: self.images_folder_path = os.getenv('IMAGES_FOLDER_PATH', f'{self.base_dir}/images') if self.categories is None: self.categories = ['True', 'Satire', 'Misleading', 'False'] # Override with environment variables for processing config env_chunk_size = os.getenv('CHUNK_SIZE') if env_chunk_size: self.chunk_size = int(env_chunk_size) env_memory_limit = os.getenv('MEMORY_LIMIT_GB') if env_memory_limit: self.memory_limit_gb = float(env_memory_limit) class FakeDitDatasetLoader: """ Handles loading and preprocessing of the Fakeddit multimodal dataset. This class implements efficient loading strategies for the large 112GB dataset, including chunked processing, memory management, and multimodal data integration. """ def __init__(self, config: DatasetConfig): self.config = config self.base_dir = Path(config.base_dir) self.train_tsv_path = Path(config.train_tsv_path) self.validation_tsv_path = Path(config.validation_tsv_path) self.test_tsv_path = Path(config.test_tsv_path) self.comments_tsv_path = Path(config.comments_tsv_path) self.images_folder_path = Path(config.images_folder_path) self.chunk_size = config.chunk_size self.memory_limit_gb = config.memory_limit_gb self.categories = config.categories # Create file path mapping self.split_file_paths = { 'train': self.train_tsv_path, 'validation': self.validation_tsv_path, 'test': self.test_tsv_path } # Validate base directory exists if not self.base_dir.exists(): logger.warning(f"Base directory not found: {self.base_dir}") logger.info(f"Initialized FakeDitDatasetLoader with base_dir: {self.base_dir}") logger.info(f"Train TSV: {self.train_tsv_path}") logger.info(f"Validation TSV: {self.validation_tsv_path}") logger.info(f"Test TSV: {self.test_tsv_path}") logger.info(f"Comments TSV: {self.comments_tsv_path}") logger.info(f"Images folder: {self.images_folder_path}") def load_multimodal_samples(self, split: str = 'train') -> pd.DataFrame: """ Load multimodal_only_samples TSV files with chunked processing. Args: split: Dataset split ('train', 'validation', 'test') Returns: DataFrame with loaded and preprocessed data Requirements: 1.1, 1.2 """ if split not in self.split_file_paths: raise ValueError(f"Invalid split: {split}. Must be one of {list(self.split_file_paths.keys())}") file_path = self.split_file_paths[split] if not file_path.exists(): raise FileNotFoundError(f"Dataset file not found: {file_path}") logger.info(f"Loading {split} dataset from {file_path}") # Check file size for information file_size_mb = file_path.stat().st_size / (1024 * 1024) logger.info(f"File size: {file_size_mb:.2f} MB") # Read in chunks to handle large file size chunks = [] total_rows = 0 try: for chunk_idx, chunk in enumerate(pd.read_csv( file_path, sep='\t', chunksize=self.chunk_size, low_memory=False )): logger.info(f"Processing chunk {chunk_idx + 1}, rows: {len(chunk)}") # Preprocess chunk processed_chunk = self._preprocess_chunk(chunk) chunks.append(processed_chunk) total_rows += len(processed_chunk) # Memory management if chunk_idx % 10 == 0: gc.collect() # Combine all chunks df = pd.concat(chunks, ignore_index=True) logger.info(f"Successfully loaded {total_rows} rows from {split} dataset") return df except Exception as e: logger.error(f"Error loading dataset {split}: {str(e)}") raise def _preprocess_chunk(self, chunk: pd.DataFrame) -> pd.DataFrame: """ Preprocess individual chunk of data. Args: chunk: Raw data chunk Returns: Preprocessed chunk Requirements: 1.4, 1.5 """ # Remove unnamed columns as specified in dataset guidelines unnamed_cols = [col for col in chunk.columns if col.startswith('Unnamed')] if unnamed_cols: chunk = chunk.drop(columns=unnamed_cols) logger.debug(f"Removed unnamed columns: {unnamed_cols}") # Clean and standardize text content from clean_title column if 'clean_title' in chunk.columns: chunk['clean_title'] = chunk['clean_title'].astype(str) chunk['clean_title'] = chunk['clean_title'].str.strip() # Replace empty strings and 'nan' strings with NaN for consistent handling chunk['clean_title'] = chunk['clean_title'].replace(['', 'nan', 'None'], np.nan) # Remove any remaining artifacts from text cleaning chunk['clean_title'] = chunk['clean_title'].str.replace(r'\s+', ' ', regex=True) # Standardize category labels if '2_way_label' in chunk.columns: chunk['authenticity_label'] = chunk['2_way_label'].map({0: 'False', 1: 'True'}) if '6_way_label' in chunk.columns: # Map 6-way labels to our 4 categories based on Fakeddit documentation # 0: True, 1: Satire, 2: Misleading, 3: True, 4: Misleading, 5: False label_mapping = { 0: 'True', # True news 1: 'Satire', # Satirical content 2: 'Misleading', # Misleading content 3: 'True', # True news (alternative category) 4: 'Misleading', # Misleading content (alternative category) 5: 'False' # False news } chunk['category_label'] = chunk['6_way_label'].map(label_mapping) # Handle missing values in key columns key_columns = ['clean_title', 'created_utc', 'score', 'num_comments'] for col in key_columns: if col in chunk.columns: if col in ['score', 'num_comments']: chunk[col] = pd.to_numeric(chunk[col], errors='coerce').fillna(0) elif col == 'created_utc': chunk[col] = pd.to_numeric(chunk[col], errors='coerce') return chunk def load_image_metadata(self) -> pd.DataFrame: """ Load and process image metadata. Returns: DataFrame with image metadata Requirements: 1.1, 1.2 """ logger.info(f"Checking for images in: {self.images_folder_path}") # Check if images folder exists if not self.images_folder_path.exists(): logger.warning(f"Images folder not found: {self.images_folder_path}") return pd.DataFrame() # Look for image files in the images folder image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp'] image_files = [] for ext in image_extensions: image_files.extend(list(self.images_folder_path.glob(f"*{ext}"))) image_files.extend(list(self.images_folder_path.glob(f"*{ext.upper()}"))) if not image_files: logger.warning("No image files found in images folder") return pd.DataFrame() logger.info(f"Found {len(image_files)} image files") # Create image metadata from found files image_metadata = [] for img_file in image_files[:1000]: # Limit to first 1000 for demo # Extract submission ID from filename (assuming format like "submission_id.jpg") submission_id = img_file.stem image_metadata.append({ 'submission_id': submission_id, 'has_image': True, 'image_url': None, # Would be populated if available 'image_path': str(img_file), 'image_size_bytes': img_file.stat().st_size if img_file.exists() else 0 }) image_df = pd.DataFrame(image_metadata) logger.info(f"Created image metadata for {len(image_df)} images") return image_df def load_comments_data(self) -> pd.DataFrame: """ Load comments data from comments.tsv. Returns: DataFrame with comments data Requirements: 1.1, 1.2 """ if not self.comments_tsv_path.exists(): logger.warning(f"Comments file not found: {self.comments_tsv_path}") return pd.DataFrame() logger.info(f"Loading comments data from {self.comments_tsv_path}") # Check file size file_size_mb = self.comments_tsv_path.stat().st_size / (1024 * 1024) logger.info(f"Comments file size: {file_size_mb:.2f} MB") try: # Load comments in chunks due to potential large size chunks = [] chunk_count = 0 for chunk in pd.read_csv(self.comments_tsv_path, sep='\t', chunksize=self.chunk_size, low_memory=False): chunk_count += 1 logger.debug(f"Loading comments chunk {chunk_count}, size: {len(chunk)}") # Basic preprocessing if 'body' in chunk.columns: chunk['body'] = chunk['body'].astype(str).str.strip() chunks.append(chunk) # Limit chunks for demo purposes (remove in production) if chunk_count >= 10: # Load only first 10 chunks for testing logger.info(f"Limited to first {chunk_count} chunks for demo") break if chunks: comments_df = pd.concat(chunks, ignore_index=True) logger.info(f"Loaded {len(comments_df)} comments from {chunk_count} chunks") else: logger.warning("No comment chunks loaded") comments_df = pd.DataFrame() return comments_df except Exception as e: logger.error(f"Error loading comments data: {str(e)}") return pd.DataFrame() def integrate_multimodal_data(self, posts_df: pd.DataFrame, image_metadata: pd.DataFrame, comments_df: pd.DataFrame) -> pd.DataFrame: """ Integrate multimodal data including text, image metadata, and comments. Args: posts_df: Main posts dataframe image_metadata: Image metadata dataframe comments_df: Comments dataframe Returns: Integrated multimodal dataframe Requirements: 1.1, 1.2 """ logger.info("Integrating multimodal data") # Start with posts as base integrated_df = posts_df.copy() # Add image metadata if available if not image_metadata.empty and 'submission_id' in image_metadata.columns: # Merge on submission ID, avoiding column conflicts if 'id' in integrated_df.columns: # Rename conflicting columns before merge image_metadata_clean = image_metadata.copy() if 'image_url' in integrated_df.columns and 'image_url' in image_metadata_clean.columns: image_metadata_clean = image_metadata_clean.rename(columns={'image_url': 'image_metadata_url'}) integrated_df = integrated_df.merge( image_metadata_clean, left_on='id', right_on='submission_id', how='left', suffixes=('', '_meta') ) # Clean up duplicate submission_id column if 'submission_id' in integrated_df.columns: integrated_df = integrated_df.drop(columns=['submission_id']) else: # Add placeholder image columns if not already present if 'has_image' not in integrated_df.columns: integrated_df['has_image'] = integrated_df.get('hasImage', False) if 'image_path' not in integrated_df.columns: integrated_df['image_path'] = None if 'image_size_bytes' not in integrated_df.columns: integrated_df['image_size_bytes'] = 0 # Aggregate comments data if available if not comments_df.empty and 'link_id' in comments_df.columns: # Aggregate comments per post comment_stats = comments_df.groupby('link_id').agg({ 'body': 'count', # Number of comments 'score': ['mean', 'sum'] if 'score' in comments_df.columns else 'count' }).reset_index() # Flatten column names comment_stats.columns = ['link_id', 'comment_count', 'avg_comment_score', 'total_comment_score'] # Merge with main dataframe if 'id' in integrated_df.columns: integrated_df = integrated_df.merge( comment_stats, left_on='id', right_on='link_id', how='left' ) logger.info(f"Integrated dataframe shape: {integrated_df.shape}") return integrated_df def create_category_segmentation(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]: """ Create category-based data segmentation for True, Satire, Misleading, and False content. Args: df: Integrated dataframe Returns: Dictionary with category-segmented dataframes Requirements: 1.4, 1.5 """ logger.info("Creating category-based data segmentation") segmented_data = {} # Use category_label if available, otherwise fall back to authenticity_label if 'category_label' in df.columns: label_col = 'category_label' elif 'authenticity_label' in df.columns: label_col = 'authenticity_label' else: logger.warning("No suitable label column found for segmentation") return segmented_data # Segment by each category for category in self.categories: category_df = df[df[label_col] == category].copy() segmented_data[category] = category_df logger.info(f"Category '{category}': {len(category_df)} samples") # Add summary statistics total_samples = sum(len(df) for df in segmented_data.values()) logger.info(f"Total segmented samples: {total_samples}") return segmented_data def load_complete_dataset(self) -> Tuple[Dict[str, pd.DataFrame], Dict[str, Dict[str, pd.DataFrame]]]: """ Load complete dataset with all splits and create category segmentation. Returns: Tuple of (split_data, segmented_data) where: - split_data: Dict with train/validation/test dataframes - segmented_data: Dict with category segmentation for each split Requirements: 1.1, 1.2, 1.4, 1.5 """ logger.info("Loading complete Fakeddit dataset") splits = ['train', 'validation', 'test'] split_data = {} segmented_data = {} # Load image metadata and comments once (shared across splits) image_metadata = self.load_image_metadata() comments_df = self.load_comments_data() for split in splits: try: logger.info(f"Processing {split} split") # Load main dataset posts_df = self.load_multimodal_samples(split) # Integrate multimodal data integrated_df = self.integrate_multimodal_data( posts_df, image_metadata, comments_df ) # Store split data split_data[split] = integrated_df # Create category segmentation segmented_data[split] = self.create_category_segmentation(integrated_df) logger.info(f"Completed processing {split} split: {len(integrated_df)} samples") except FileNotFoundError: logger.warning(f"Split {split} not found, skipping") continue except Exception as e: logger.error(f"Error processing {split}: {str(e)}") continue logger.info("Dataset loading completed") return split_data, segmented_data def create_dataset_loader(**kwargs) -> FakeDitDatasetLoader: """ Factory function to create a dataset loader with configuration. Args: **kwargs: Configuration parameters (will use .env defaults if not provided) Returns: Configured FakeDitDatasetLoader instance """ config = DatasetConfig(**kwargs) return FakeDitDatasetLoader(config)